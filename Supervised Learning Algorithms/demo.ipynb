{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love\n",
      "gose\n",
      "oh\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "class RepeatReplacer:\n",
    "\n",
    "  def __init__(self):\n",
    "    self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    self.repl = r'\\1\\2\\3'\n",
    "\n",
    "  def replace(self, word):\n",
    "    repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "\n",
    "    if repl_word != word:\n",
    "      return self.replace(repl_word)\n",
    "    else:\n",
    "      return repl_word\n",
    "    \n",
    "replacer = RepeatReplacer()\n",
    "text = replacer.replace(\"looooove\")\n",
    "text1 = replacer.replace(\"gooose\")\n",
    "text2 = replacer.replace(\"ooooooohh\")\n",
    "print(text)\n",
    "print(text1)\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gose\n",
      "oh\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "class RepeatReplacer:\n",
    "\n",
    "  def __init__(self):\n",
    "    self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    self.repl = r'\\1\\2\\3'\n",
    "\n",
    "  def replace(self, word):\n",
    "    if wordnet.synsets(word):\n",
    "      return word\n",
    "    repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "\n",
    "    if repl_word != word:\n",
    "      return self.replace(repl_word)\n",
    "    else:\n",
    "      return repl_word\n",
    "    \n",
    "text = replacer.replace(\"goose\")\n",
    "text1 = replacer.replace(\"oooooh\")\n",
    "print(text)\n",
    "print(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Birthday'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WordReplacer:\n",
    "  def __init__(self,word_map):\n",
    "    self.word_map = word_map\n",
    "\n",
    "  def replace(self,word):\n",
    "    return self.word_map.get(word,word)\n",
    "  \n",
    "replacer = WordReplacer({\"bday\" : \"Birthday\"})\n",
    "replacer.replace(\"bday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m       word_map[word] \u001b[38;5;241m=\u001b[39m syn \n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28msuper\u001b[39m(CsvWordReplacer, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(word_map)\n\u001b[1;32m---> 12\u001b[0m replacer \u001b[38;5;241m=\u001b[39m \u001b[43mCsvWordReplacer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWordnetSynonyms.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m replacer\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhappy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m, in \u001b[0;36mCsvWordReplacer.__init__\u001b[1;34m(self, fname)\u001b[0m\n\u001b[0;32m      6\u001b[0m word_map \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m csv\u001b[38;5;241m.\u001b[39mreader(\u001b[38;5;28mopen\u001b[39m(fname)):\n\u001b[1;32m----> 8\u001b[0m   word, syn  \u001b[38;5;241m=\u001b[39m line\n\u001b[0;32m      9\u001b[0m   word_map[word] \u001b[38;5;241m=\u001b[39m syn \n\u001b[0;32m     10\u001b[0m \u001b[38;5;28msuper\u001b[39m(CsvWordReplacer, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(word_map)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "class CsvWordReplacer(WordReplacer):\n",
    "\n",
    "  def __init__(self,fname):\n",
    "    word_map = {}\n",
    "    for line in csv.reader(open(fname)):\n",
    "      word, syn  = line\n",
    "      word_map[word] = syn \n",
    "    super(CsvWordReplacer, self).__init__(word_map)\n",
    "\n",
    "replacer = CsvWordReplacer(\"WordnetSynonyms.csv\")\n",
    "replacer.replace(\"happy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boyfriend\n",
      "birthday\n",
      "please\n",
      "best friend forever\n",
      "could\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "class CsvWordReplacer(WordReplacer):\n",
    "    def __init__(self, fname):\n",
    "        word_map = {}\n",
    "        with open(fname, 'r') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            for row in reader:\n",
    "                word = row['Word']\n",
    "                synonyms = row['Synonyms']\n",
    "                word_map[word] = synonyms\n",
    "        super().__init__(word_map)\n",
    "\n",
    "replacer = CsvWordReplacer(\"Synonyms.csv\")\n",
    "\n",
    "text = replacer.replace(\"bf\")\n",
    "text1 = replacer.replace(\"bday\")\n",
    "text2 = replacer.replace(\"pls\")\n",
    "text3 = replacer.replace(\"bff\")\n",
    "text4 = replacer.replace(\"cud\")\n",
    "\n",
    "print(text)\n",
    "print(text1)\n",
    "print(text2)\n",
    "print(text3)\n",
    "print(text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms.csv' file\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "# Create a list of data\n",
    "data = [\n",
    "    [\"bday\", \"birthday\"],\n",
    "    [\"bff\", \"best friend forever\"],\n",
    "    [\"lol\", \"laugh out loud\"],\n",
    "    [\"gr8\", \"great\"],\n",
    "    [\"gonna\", \"going to\"],\n",
    "    [\"wanna\", \"want to\"],\n",
    "    [\"cuz\", \"because\"],\n",
    "    [\"thx\", \"thanks\"],\n",
    "    [\"pls\",\"please\"],\n",
    "    [\"u\", \"you\"],\n",
    "    [\"2day\",\"today\"],\n",
    "    [\"tmrw\",\"tomorrow\"],\n",
    "    [\"wknd\",\"weekend\"],\n",
    "    [\"gf\",\"girlfriend\"],\n",
    "    [\"bf\",\"boyfriend\"],\n",
    "    [\"wut\",\"what\"],\n",
    "    [\"thru\",\"through\"],\n",
    "    [\"cud\",\"could\"],\n",
    "    [\"pls\",\"please\"],\n",
    "    [\"tho\",\"though\"],\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"Word\", \"Synonyms\"])\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df.to_csv(\"Synonyms.csv\", index=False)\n",
    "\n",
    "\n",
    "print(\"Synonyms.csv' file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"let's\", 'beautify', 'our', 'code']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "class AntonymReplacer:\n",
    "\n",
    "    def replace(self, word, pos=None):\n",
    "        antonyms = set()\n",
    "\n",
    "        for syn in wordnet.synsets(word, pos=pos):\n",
    "            for lemma in syn.lemmas():\n",
    "                for antonym in lemma.antonyms():\n",
    "                    antonyms.add(antonym.name())\n",
    "\n",
    "        if len(antonyms) == 1:\n",
    "            return antonyms.pop()\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def replace_negations(self, sent):\n",
    "        i, l = 0, len(sent)\n",
    "        words = []\n",
    "        while i < l:\n",
    "            word = sent[i]\n",
    "            if word == \"not\" and i + 1 < l:\n",
    "                ant = self.replace(sent[i + 1])\n",
    "                if ant:\n",
    "                    words.append(ant)\n",
    "                    i += 2\n",
    "                    continue\n",
    "            words.append(word)\n",
    "            i += 1\n",
    "        return words\n",
    "\n",
    "\n",
    "replacer = AntonymReplacer()\n",
    "\n",
    "sent = [\"let's\", 'not', 'uglify', 'our', 'code']\n",
    "\n",
    "\n",
    "print(replacer.replace_negations(sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Inc. _ 383\n",
      "Steve Jobs _ 380\n",
      "Steve Wozniak _ 380\n",
      "California _ 384\n",
      "April 1, 1976 _ 391\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"Apple Inc. was founded by Steve Jobs and Steve Wozniak in California on April 1, 1976.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "  print(ent.text, \"_\", ent.label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'function' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ments\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(color_map\u001b[38;5;241m.\u001b[39mkeys()), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyle\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolors\u001b[39m\u001b[38;5;124m\"\u001b[39m: style_entity_label}\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Render the entities with words using displacy\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[43mdisplacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mistr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\displacy\\__init__.py:56\u001b[0m, in \u001b[0;36mrender\u001b[1;34m(docs, style, page, minify, jupyter, options, manual)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE096)\n\u001b[0;32m     55\u001b[0m renderer_func, converter \u001b[38;5;241m=\u001b[39m factories[style]\n\u001b[1;32m---> 56\u001b[0m renderer \u001b[38;5;241m=\u001b[39m \u001b[43mrenderer_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m parsed \u001b[38;5;241m=\u001b[39m [converter(doc, options) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m manual \u001b[38;5;28;01melse\u001b[39;00m docs  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manual:\n",
      "File \u001b[1;32mc:\\Users\\mistr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\displacy\\render.py:524\u001b[0m, in \u001b[0;36mEntityRenderer.__init__\u001b[1;34m(self, options)\u001b[0m\n\u001b[0;32m    522\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE925\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m(user_color)))\n\u001b[0;32m    523\u001b[0m     colors\u001b[38;5;241m.\u001b[39mupdate(user_color)\n\u001b[1;32m--> 524\u001b[0m \u001b[43mcolors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_color \u001b[38;5;241m=\u001b[39m DEFAULT_ENTITY_COLOR\n\u001b[0;32m    526\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolors \u001b[38;5;241m=\u001b[39m {label\u001b[38;5;241m.\u001b[39mupper(): color \u001b[38;5;28;01mfor\u001b[39;00m label, color \u001b[38;5;129;01min\u001b[39;00m colors\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[1;31mTypeError\u001b[0m: 'function' object is not iterable"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from colorama import Fore, Style\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define your text\n",
    "text = \"Apple Inc. was founded by Steve Jobs and Steve Wozniak in California on April 1, 1976.\"\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Define color mappings for entity types\n",
    "color_map = {\n",
    "    \"PERSON\": Fore.BLUE,\n",
    "    \"ORG\": Fore.GREEN,\n",
    "    \"GPE\": Fore.RED,\n",
    "    \"DATE\": Fore.YELLOW,\n",
    "    # Add more entity types and corresponding colors if needed\n",
    "}\n",
    "\n",
    "# Iterate through the entities and print them with color\n",
    "for ent in doc.ents:\n",
    "    full_entity = text[ent.start_char:ent.end_char]\n",
    "    label = ent.label_\n",
    "    color = color_map.get(label, Fore.WHITE)  # Default color is white\n",
    "    print(color + full_entity + \" [\" + label + \"]\" + Style.RESET_ALL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple Inc.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " was founded by \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Steve Jobs\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Steve Wozniak\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    California\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " on \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    April 1, 1976\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Define your text\n",
    "text = \"Apple Inc. was founded by Steve Jobs and Steve Wozniak in California on April 1, 1976.\"\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# # Define color mappings for entity types\n",
    "# color_map = {\n",
    "#     \"PERSON\": \"#00FFFF\",  # Cyan\n",
    "#     \"ORG\": \"#00FF00\",     # Green\n",
    "#     \"GPE\": \"#FF0000\",     # Red\n",
    "#     \"DATE\": \"#FFFF00\",    # Yellow\n",
    "#     \"TIME\": \"#FF1493\",    # DeepPink\n",
    "#     \"MONEY\": \"#FFD700\",   # Gold\n",
    "#     \"PERCENT\": \"#9370DB\", # MediumPurple\n",
    "#     \"QUANTITY\": \"#20B2AA\",# LightSeaGreen\n",
    "#     \"ORDINAL\": \"#FF8C00\", # DarkOrange\n",
    "#     \"CARDINAL\": \"#CD5C5C\" # IndianRed\n",
    "# }\n",
    "\n",
    "# Options for displacy rendering\n",
    "# options = {\"ents\": list(color_map.keys()), \"colors\": color_map}\n",
    "\n",
    "# Render the entities with words using displacy\n",
    "displacy.render(doc, style=\"ent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple Inc.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " was founded by \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Steve Jobs\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Steve Wozniak\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    California\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " on \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    April 1, 1976\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Define your text\n",
    "text = \"Apple Inc. was founded by Steve Jobs and Steve Wozniak in California on April 1, 1976.\"\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Render the entities with words using displacy\n",
    "displacy.render(doc, style=\"ent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple Inc.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " was founded by \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Steve Jobs\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Steve Wozniak\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    California\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " on \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    April 1 , 1976\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Define your text\n",
    "text = \"Apple Inc. was founded by Steve Jobs and Steve Wozniak in California on April 1, 1976.\"\n",
    "\n",
    "# Tokenize the text using NLTK\n",
    "word_tokens = word_tokenize(text)\n",
    "\n",
    "# Join the tokens back into a single string\n",
    "text = \" \".join(word_tokens)\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Render the entities with words using displacy\n",
    "displacy.render(doc, style=\"ent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple Inc.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " was founded by \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Steve Jobs\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Steve Wozniak\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    California\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " on \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    April 1, 1976\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define your text\n",
    "text = \"Apple Inc. was founded by Steve Jobs and Steve Wozniak in California on April 1, 1976.\"\n",
    "\n",
    "# Tokenization: Break down the input text into individual words or tokens\n",
    "tokens = nlp(text)\n",
    "\n",
    "# Part-of-Speech (POS) Tagging: Assign grammatical roles to each token\n",
    "pos_tags = [(token.text, token.pos_) for token in tokens]\n",
    "\n",
    "# Named Entity Recognition: Identify named entities in the text\n",
    "named_entities = [(entity.text, entity.label_) for entity in tokens.ents]\n",
    "\n",
    "# Entity Classification: Classify named entities into predefined categories\n",
    "entity_categories = [(entity.text, entity.label_) for entity in tokens.ents]\n",
    "\n",
    "# Visualize Named Entities\n",
    "displacy.render(tokens, style=\"ent\", jupyter=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple Inc.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is a multinational technology company headquartered in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Cupertino\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    California\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    USA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ". Founded on \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    April 1, 1976\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", by \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Steve Jobs\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Steve Wozniak\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", and \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Ronald Wayne\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " has grown to become one of the world's leading innovators in consumer electronics, software, and services. </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "   \n",
    "content = \"Apple Inc. is a multinational technology company headquartered in Cupertino, California, USA. Founded on April 1, 1976, by Steve Jobs, Steve Wozniak, and Ronald Wayne, Apple has grown to become one of the world's leading innovators in consumer electronics, software, and services. \"\n",
    " \n",
    "doc = nlp(content)\n",
    "\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple Inc.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is a global technology company known for its innovative products like the iPhone , \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    iPad\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " , and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mac\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " computers . Founded in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1976\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " by \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Steve Jobs\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " , \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Steve Wozniak\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " , and \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Ronald Wayne\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " , \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " has a reputation for sleek design , seamless integration of hardware and software , and strong marketing . It offers a range of services such as iCloud , \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple Music\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " , and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple Pay\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " , along with a commitment to environmental sustainability . \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " 's impact on technology and culture is profound , shaping the way we interact with devices and consume media .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Define your text\n",
    "text = \"Apple Inc. is a global technology company known for its innovative products like the iPhone, iPad, and Mac computers. Founded in 1976 by Steve Jobs, Steve Wozniak, and Ronald Wayne, Apple has a reputation for sleek design, seamless integration of hardware and software, and strong marketing. It offers a range of services such as iCloud, Apple Music, and Apple Pay, along with a commitment to environmental sustainability. Apple's impact on technology and culture is profound, shaping the way we interact with devices and consume media.\"\n",
    "\n",
    "# Tokenize the text using NLTK\n",
    "word_tokens = word_tokenize(text)\n",
    "\n",
    "# Join the tokens back into a single string\n",
    "text = \" \".join(word_tokens)\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Render the entities with words using displacy\n",
    "displacy.render(doc, style=\"ent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Named Entity Recognition</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Your text\n",
    "text = \"Named Entity Recognition\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Define custom color scheme\n",
    "colors = {\"ORG\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\"}\n",
    "\n",
    "# Visualize named entities with custom colors\n",
    "options = {\"ents\": [\"ORG\"], \"colors\": colors}\n",
    "displacy.render(doc, style=\"ent\", jupyter=True, options=options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMPLOYEE_ID</th>\n",
       "      <th>FIRST_NAME</th>\n",
       "      <th>LAST_NAME</th>\n",
       "      <th>EMAIL</th>\n",
       "      <th>PHONE_NUMBER</th>\n",
       "      <th>HIRE_DATE</th>\n",
       "      <th>JOB_ID</th>\n",
       "      <th>SALARY</th>\n",
       "      <th>COMMISSION_PCT</th>\n",
       "      <th>MANAGER_ID</th>\n",
       "      <th>DEPARTMENT_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>198</td>\n",
       "      <td>Donald</td>\n",
       "      <td>OConnell</td>\n",
       "      <td>DOCONNEL</td>\n",
       "      <td>650.507.9833</td>\n",
       "      <td>21-JUN-07</td>\n",
       "      <td>SH_CLERK</td>\n",
       "      <td>2600</td>\n",
       "      <td>-</td>\n",
       "      <td>124</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>199</td>\n",
       "      <td>Douglas</td>\n",
       "      <td>Grant</td>\n",
       "      <td>DGRANT</td>\n",
       "      <td>650.507.9844</td>\n",
       "      <td>13-JAN-08</td>\n",
       "      <td>SH_CLERK</td>\n",
       "      <td>2600</td>\n",
       "      <td>-</td>\n",
       "      <td>124</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200</td>\n",
       "      <td>Jennifer</td>\n",
       "      <td>Whalen</td>\n",
       "      <td>JWHALEN</td>\n",
       "      <td>515.123.4444</td>\n",
       "      <td>17-SEP-03</td>\n",
       "      <td>AD_ASST</td>\n",
       "      <td>4400</td>\n",
       "      <td>-</td>\n",
       "      <td>101</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201</td>\n",
       "      <td>Michael</td>\n",
       "      <td>Hartstein</td>\n",
       "      <td>MHARTSTE</td>\n",
       "      <td>515.123.5555</td>\n",
       "      <td>17-FEB-04</td>\n",
       "      <td>MK_MAN</td>\n",
       "      <td>13000</td>\n",
       "      <td>-</td>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>202</td>\n",
       "      <td>Pat</td>\n",
       "      <td>Fay</td>\n",
       "      <td>PFAY</td>\n",
       "      <td>603.123.6666</td>\n",
       "      <td>17-AUG-05</td>\n",
       "      <td>MK_REP</td>\n",
       "      <td>6000</td>\n",
       "      <td>-</td>\n",
       "      <td>201</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EMPLOYEE_ID FIRST_NAME  LAST_NAME     EMAIL  PHONE_NUMBER  HIRE_DATE  \\\n",
       "0          198     Donald   OConnell  DOCONNEL  650.507.9833  21-JUN-07   \n",
       "1          199    Douglas      Grant    DGRANT  650.507.9844  13-JAN-08   \n",
       "2          200   Jennifer     Whalen   JWHALEN  515.123.4444  17-SEP-03   \n",
       "3          201    Michael  Hartstein  MHARTSTE  515.123.5555  17-FEB-04   \n",
       "4          202        Pat        Fay      PFAY  603.123.6666  17-AUG-05   \n",
       "\n",
       "     JOB_ID  SALARY COMMISSION_PCT MANAGER_ID  DEPARTMENT_ID  \n",
       "0  SH_CLERK    2600             -         124             50  \n",
       "1  SH_CLERK    2600             -         124             50  \n",
       "2   AD_ASST    4400             -         101             10  \n",
       "3    MK_MAN   13000             -         100             20  \n",
       "4    MK_REP    6000             -         201             20  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"employees.csv\")\n",
    "\n",
    "data.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DOCONNEL' 'DGRANT' 'JWHALEN' 'MHARTSTE' 'PFAY' 'SMAVRIS' 'HBAER'\n",
      " 'SHIGGINS' 'WGIETZ' 'SKING' 'NKOCHHAR' 'LDEHAAN' 'AHUNOLD' 'BERNST'\n",
      " 'DAUSTIN' 'VPATABAL' 'DLORENTZ' 'NGREENBE' 'DFAVIET' 'JCHEN' 'ISCIARRA'\n",
      " 'JMURMAN' 'LPOPP' 'DRAPHEAL' 'AKHOO' 'SBAIDA' 'STOBIAS' 'GHIMURO'\n",
      " 'KCOLMENA' 'MWEISS' 'AFRIPP' 'PKAUFLIN' 'SVOLLMAN' 'KMOURGOS' 'JNAYER'\n",
      " 'IMIKKILI' 'JLANDRY' 'SMARKLE' 'LBISSOT' 'MATKINSO' 'JAMRLOW' 'TJOLSON'\n",
      " 'JMALLIN' 'MROGERS' 'KGEE' 'HPHILTAN' 'RLADWIG' 'SSTILES' 'JSEO' 'JPATEL']\n",
      "[198 199 200 201 202 203 204 205 206 100 101 102 103 104 105 106 107 108\n",
      " 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n",
      " 127 128 129 130 131 132 133 134 135 136 137 138 139 140]\n"
     ]
    }
   ],
   "source": [
    "print(data[\"EMAIL\"].unique())\n",
    "print(data[\"EMPLOYEE_ID\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EMPLOYEE_ID\n",
       "198    0.02\n",
       "128    0.02\n",
       "118    0.02\n",
       "119    0.02\n",
       "120    0.02\n",
       "121    0.02\n",
       "122    0.02\n",
       "123    0.02\n",
       "124    0.02\n",
       "125    0.02\n",
       "126    0.02\n",
       "127    0.02\n",
       "129    0.02\n",
       "199    0.02\n",
       "130    0.02\n",
       "131    0.02\n",
       "132    0.02\n",
       "133    0.02\n",
       "134    0.02\n",
       "135    0.02\n",
       "136    0.02\n",
       "137    0.02\n",
       "138    0.02\n",
       "139    0.02\n",
       "117    0.02\n",
       "116    0.02\n",
       "115    0.02\n",
       "114    0.02\n",
       "200    0.02\n",
       "201    0.02\n",
       "202    0.02\n",
       "203    0.02\n",
       "204    0.02\n",
       "205    0.02\n",
       "206    0.02\n",
       "100    0.02\n",
       "101    0.02\n",
       "102    0.02\n",
       "103    0.02\n",
       "104    0.02\n",
       "105    0.02\n",
       "106    0.02\n",
       "107    0.02\n",
       "108    0.02\n",
       "109    0.02\n",
       "110    0.02\n",
       "111    0.02\n",
       "112    0.02\n",
       "113    0.02\n",
       "140    0.02\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"EMPLOYEE_ID\"].value_counts(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      LAST_NAME     EMAIL  PHONE_NUMBER  HIRE_DATE      JOB_ID  SALARY  \\\n",
      "0      OConnell  DOCONNEL  650.507.9833  21-JUN-07    SH_CLERK    2600   \n",
      "1         Grant    DGRANT  650.507.9844  13-JAN-08    SH_CLERK    2600   \n",
      "2        Whalen   JWHALEN  515.123.4444  17-SEP-03     AD_ASST    4400   \n",
      "3     Hartstein  MHARTSTE  515.123.5555  17-FEB-04      MK_MAN   13000   \n",
      "4           Fay      PFAY  603.123.6666  17-AUG-05      MK_REP    6000   \n",
      "5        Mavris   SMAVRIS  515.123.7777  07-JUN-02      HR_REP    6500   \n",
      "6          Baer     HBAER  515.123.8888  07-JUN-02      PR_REP   10000   \n",
      "7       Higgins  SHIGGINS  515.123.8080  07-JUN-02      AC_MGR   12008   \n",
      "8         Gietz    WGIETZ  515.123.8181  07-JUN-02  AC_ACCOUNT    8300   \n",
      "9          King     SKING  515.123.4567  17-JUN-03     AD_PRES   24000   \n",
      "10      Kochhar  NKOCHHAR  515.123.4568  21-SEP-05       AD_VP   17000   \n",
      "11      De Haan   LDEHAAN  515.123.4569  13-JAN-01       AD_VP   17000   \n",
      "12       Hunold   AHUNOLD  590.423.4567  03-JAN-06     IT_PROG    9000   \n",
      "13        Ernst    BERNST  590.423.4568  21-MAY-07     IT_PROG    6000   \n",
      "14       Austin   DAUSTIN  590.423.4569  25-JUN-05     IT_PROG    4800   \n",
      "15    Pataballa  VPATABAL  590.423.4560  05-FEB-06     IT_PROG    4800   \n",
      "16      Lorentz  DLORENTZ  590.423.5567  07-FEB-07     IT_PROG    4200   \n",
      "17    Greenberg  NGREENBE  515.124.4569  17-AUG-02      FI_MGR   12008   \n",
      "18       Faviet   DFAVIET  515.124.4169  16-AUG-02  FI_ACCOUNT    9000   \n",
      "19         Chen     JCHEN  515.124.4269  28-SEP-05  FI_ACCOUNT    8200   \n",
      "20      Sciarra  ISCIARRA  515.124.4369  30-SEP-05  FI_ACCOUNT    7700   \n",
      "21        Urman   JMURMAN  515.124.4469  07-MAR-06  FI_ACCOUNT    7800   \n",
      "22         Popp     LPOPP  515.124.4567  07-DEC-07  FI_ACCOUNT    6900   \n",
      "23     Raphaely  DRAPHEAL  515.127.4561  07-DEC-02      PU_MAN   11000   \n",
      "24         Khoo     AKHOO  515.127.4562  18-MAY-03    PU_CLERK    3100   \n",
      "25        Baida    SBAIDA  515.127.4563  24-DEC-05    PU_CLERK    2900   \n",
      "26       Tobias   STOBIAS  515.127.4564  24-JUL-05    PU_CLERK    2800   \n",
      "27       Himuro   GHIMURO  515.127.4565  15-NOV-06    PU_CLERK    2600   \n",
      "28   Colmenares  KCOLMENA  515.127.4566  10-AUG-07    PU_CLERK    2500   \n",
      "29        Weiss    MWEISS  650.123.1234  18-JUL-04      ST_MAN    8000   \n",
      "30        Fripp    AFRIPP  650.123.2234  10-APR-05      ST_MAN    8200   \n",
      "31     Kaufling  PKAUFLIN  650.123.3234  01-MAY-03      ST_MAN    7900   \n",
      "32      Vollman  SVOLLMAN  650.123.4234  10-OCT-05      ST_MAN    6500   \n",
      "33      Mourgos  KMOURGOS  650.123.5234  16-NOV-07      ST_MAN    5800   \n",
      "34        Nayer    JNAYER  650.124.1214  16-JUL-05    ST_CLERK    3200   \n",
      "35  Mikkilineni  IMIKKILI  650.124.1224  28-SEP-06    ST_CLERK    2700   \n",
      "36       Landry   JLANDRY  650.124.1334  14-JAN-07    ST_CLERK    2400   \n",
      "37       Markle   SMARKLE  650.124.1434  08-MAR-08    ST_CLERK    2200   \n",
      "38       Bissot   LBISSOT  650.124.5234  20-AUG-05    ST_CLERK    3300   \n",
      "39     Atkinson  MATKINSO  650.124.6234  30-OCT-05    ST_CLERK    2800   \n",
      "40       Marlow   JAMRLOW  650.124.7234  16-FEB-05    ST_CLERK    2500   \n",
      "41        Olson   TJOLSON  650.124.8234  10-APR-07    ST_CLERK    2100   \n",
      "42       Mallin   JMALLIN  650.127.1934  14-JUN-04    ST_CLERK    3300   \n",
      "43       Rogers   MROGERS  650.127.1834  26-AUG-06    ST_CLERK    2900   \n",
      "44          Gee      KGEE  650.127.1734  12-DEC-07    ST_CLERK    2400   \n",
      "45   Philtanker  HPHILTAN  650.127.1634  06-FEB-08    ST_CLERK    2200   \n",
      "46       Ladwig   RLADWIG  650.121.1234  14-JUL-03    ST_CLERK    3600   \n",
      "47       Stiles   SSTILES  650.121.2034  26-OCT-05    ST_CLERK    3200   \n",
      "48          Seo      JSEO  650.121.2019  12-FEB-06    ST_CLERK    2700   \n",
      "49        Patel    JPATEL  650.121.1834  06-APR-06    ST_CLERK    2500   \n",
      "\n",
      "   COMMISSION_PCT MANAGER_ID  DEPARTMENT_ID  FIRST_NAME_Adam  ...  \\\n",
      "0              -         124             50            False  ...   \n",
      "1              -         124             50            False  ...   \n",
      "2              -         101             10            False  ...   \n",
      "3              -         100             20            False  ...   \n",
      "4              -         201             20            False  ...   \n",
      "5              -         101             40            False  ...   \n",
      "6              -         101             70            False  ...   \n",
      "7              -         101            110            False  ...   \n",
      "8              -         205            110            False  ...   \n",
      "9              -          -              90            False  ...   \n",
      "10             -         100             90            False  ...   \n",
      "11             -         100             90            False  ...   \n",
      "12             -         102             60            False  ...   \n",
      "13             -         103             60            False  ...   \n",
      "14             -         103             60            False  ...   \n",
      "15             -         103             60            False  ...   \n",
      "16             -         103             60            False  ...   \n",
      "17             -         101            100            False  ...   \n",
      "18             -         108            100            False  ...   \n",
      "19             -         108            100            False  ...   \n",
      "20             -         108            100            False  ...   \n",
      "21             -         108            100            False  ...   \n",
      "22             -         108            100            False  ...   \n",
      "23             -         100             30            False  ...   \n",
      "24             -         114             30            False  ...   \n",
      "25             -         114             30            False  ...   \n",
      "26             -         114             30            False  ...   \n",
      "27             -         114             30            False  ...   \n",
      "28             -         114             30            False  ...   \n",
      "29             -         100             50            False  ...   \n",
      "30             -         100             50             True  ...   \n",
      "31             -         100             50            False  ...   \n",
      "32             -         100             50            False  ...   \n",
      "33             -         100             50            False  ...   \n",
      "34             -         120             50            False  ...   \n",
      "35             -         120             50            False  ...   \n",
      "36             -         120             50            False  ...   \n",
      "37             -         120             50            False  ...   \n",
      "38             -         121             50            False  ...   \n",
      "39             -         121             50            False  ...   \n",
      "40             -         121             50            False  ...   \n",
      "41             -         121             50            False  ...   \n",
      "42             -         122             50            False  ...   \n",
      "43             -         122             50            False  ...   \n",
      "44             -         122             50            False  ...   \n",
      "45             -         122             50            False  ...   \n",
      "46             -         123             50            False  ...   \n",
      "47             -         123             50            False  ...   \n",
      "48             -         123             50            False  ...   \n",
      "49             -         123             50            False  ...   \n",
      "\n",
      "    EMPLOYEE_ID_140  EMPLOYEE_ID_198  EMPLOYEE_ID_199  EMPLOYEE_ID_200  \\\n",
      "0             False             True            False            False   \n",
      "1             False            False             True            False   \n",
      "2             False            False            False             True   \n",
      "3             False            False            False            False   \n",
      "4             False            False            False            False   \n",
      "5             False            False            False            False   \n",
      "6             False            False            False            False   \n",
      "7             False            False            False            False   \n",
      "8             False            False            False            False   \n",
      "9             False            False            False            False   \n",
      "10            False            False            False            False   \n",
      "11            False            False            False            False   \n",
      "12            False            False            False            False   \n",
      "13            False            False            False            False   \n",
      "14            False            False            False            False   \n",
      "15            False            False            False            False   \n",
      "16            False            False            False            False   \n",
      "17            False            False            False            False   \n",
      "18            False            False            False            False   \n",
      "19            False            False            False            False   \n",
      "20            False            False            False            False   \n",
      "21            False            False            False            False   \n",
      "22            False            False            False            False   \n",
      "23            False            False            False            False   \n",
      "24            False            False            False            False   \n",
      "25            False            False            False            False   \n",
      "26            False            False            False            False   \n",
      "27            False            False            False            False   \n",
      "28            False            False            False            False   \n",
      "29            False            False            False            False   \n",
      "30            False            False            False            False   \n",
      "31            False            False            False            False   \n",
      "32            False            False            False            False   \n",
      "33            False            False            False            False   \n",
      "34            False            False            False            False   \n",
      "35            False            False            False            False   \n",
      "36            False            False            False            False   \n",
      "37            False            False            False            False   \n",
      "38            False            False            False            False   \n",
      "39            False            False            False            False   \n",
      "40            False            False            False            False   \n",
      "41            False            False            False            False   \n",
      "42            False            False            False            False   \n",
      "43            False            False            False            False   \n",
      "44            False            False            False            False   \n",
      "45            False            False            False            False   \n",
      "46            False            False            False            False   \n",
      "47            False            False            False            False   \n",
      "48            False            False            False            False   \n",
      "49             True            False            False            False   \n",
      "\n",
      "    EMPLOYEE_ID_201  EMPLOYEE_ID_202  EMPLOYEE_ID_203  EMPLOYEE_ID_204  \\\n",
      "0             False            False            False            False   \n",
      "1             False            False            False            False   \n",
      "2             False            False            False            False   \n",
      "3              True            False            False            False   \n",
      "4             False             True            False            False   \n",
      "5             False            False             True            False   \n",
      "6             False            False            False             True   \n",
      "7             False            False            False            False   \n",
      "8             False            False            False            False   \n",
      "9             False            False            False            False   \n",
      "10            False            False            False            False   \n",
      "11            False            False            False            False   \n",
      "12            False            False            False            False   \n",
      "13            False            False            False            False   \n",
      "14            False            False            False            False   \n",
      "15            False            False            False            False   \n",
      "16            False            False            False            False   \n",
      "17            False            False            False            False   \n",
      "18            False            False            False            False   \n",
      "19            False            False            False            False   \n",
      "20            False            False            False            False   \n",
      "21            False            False            False            False   \n",
      "22            False            False            False            False   \n",
      "23            False            False            False            False   \n",
      "24            False            False            False            False   \n",
      "25            False            False            False            False   \n",
      "26            False            False            False            False   \n",
      "27            False            False            False            False   \n",
      "28            False            False            False            False   \n",
      "29            False            False            False            False   \n",
      "30            False            False            False            False   \n",
      "31            False            False            False            False   \n",
      "32            False            False            False            False   \n",
      "33            False            False            False            False   \n",
      "34            False            False            False            False   \n",
      "35            False            False            False            False   \n",
      "36            False            False            False            False   \n",
      "37            False            False            False            False   \n",
      "38            False            False            False            False   \n",
      "39            False            False            False            False   \n",
      "40            False            False            False            False   \n",
      "41            False            False            False            False   \n",
      "42            False            False            False            False   \n",
      "43            False            False            False            False   \n",
      "44            False            False            False            False   \n",
      "45            False            False            False            False   \n",
      "46            False            False            False            False   \n",
      "47            False            False            False            False   \n",
      "48            False            False            False            False   \n",
      "49            False            False            False            False   \n",
      "\n",
      "    EMPLOYEE_ID_205  EMPLOYEE_ID_206  \n",
      "0             False            False  \n",
      "1             False            False  \n",
      "2             False            False  \n",
      "3             False            False  \n",
      "4             False            False  \n",
      "5             False            False  \n",
      "6             False            False  \n",
      "7              True            False  \n",
      "8             False             True  \n",
      "9             False            False  \n",
      "10            False            False  \n",
      "11            False            False  \n",
      "12            False            False  \n",
      "13            False            False  \n",
      "14            False            False  \n",
      "15            False            False  \n",
      "16            False            False  \n",
      "17            False            False  \n",
      "18            False            False  \n",
      "19            False            False  \n",
      "20            False            False  \n",
      "21            False            False  \n",
      "22            False            False  \n",
      "23            False            False  \n",
      "24            False            False  \n",
      "25            False            False  \n",
      "26            False            False  \n",
      "27            False            False  \n",
      "28            False            False  \n",
      "29            False            False  \n",
      "30            False            False  \n",
      "31            False            False  \n",
      "32            False            False  \n",
      "33            False            False  \n",
      "34            False            False  \n",
      "35            False            False  \n",
      "36            False            False  \n",
      "37            False            False  \n",
      "38            False            False  \n",
      "39            False            False  \n",
      "40            False            False  \n",
      "41            False            False  \n",
      "42            False            False  \n",
      "43            False            False  \n",
      "44            False            False  \n",
      "45            False            False  \n",
      "46            False            False  \n",
      "47            False            False  \n",
      "48            False            False  \n",
      "49            False            False  \n",
      "\n",
      "[50 rows x 104 columns]\n"
     ]
    }
   ],
   "source": [
    "one_hot = pd.get_dummies(data, columns=[\"FIRST_NAME\",\"EMPLOYEE_ID\"])\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Blue  Green    Red\n",
      "0  False  False   True\n",
      "1   True  False  False\n",
      "2  False   True  False\n",
      "3  False  False   True\n",
      "4  False   True  False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'Color': ['Red', 'Blue', 'Green', 'Red', 'Green']}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "one_hot = pd.get_dummies(df[\"Color\"])\n",
    "\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Fruit_Apple  Fruit_Banana  Fruit_Mango  Fruit_Orange\n",
      "0            1             0            0             0\n",
      "1            0             0            0             1\n",
      "2            0             1            0             0\n",
      "3            1             0            0             0\n",
      "4            0             0            1             0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'Fruit': ['Apple', 'Orange', 'Banana', 'Apple',\"Mango\"]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "ohm = pd.get_dummies(df,columns=[\"Fruit\"],dtype=int)\n",
    "print(ohm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fruit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Orange</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Banana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mango</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Fruit\n",
       "0   Apple\n",
       "1  Orange\n",
       "2  Banana\n",
       "3   Apple\n",
       "4   Mango"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'Fruit': ['Apple', 'Orange', 'Banana', 'Apple',\"Mango\"]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot Encoding using Pandas Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fruit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Orange</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Banana</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Fruit\n",
       "0   Apple\n",
       "1  Orange\n",
       "2  Banana"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'Fruit': ['Apple', 'Orange', 'Banana']}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Fruit_Apple  Fruit_Banana  Fruit_Orange\n",
      "0            1             0             0\n",
      "1            0             0             1\n",
      "2            0             1             0\n"
     ]
    }
   ],
   "source": [
    "ohm = pd.get_dummies(df,columns=[\"Fruit\"],dtype=int)\n",
    "print(ohm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Color  Fruits_Apple  Fruits_Banana  Fruits_Orange\n",
      "0     Red             1              0              0\n",
      "1  Orange             0              0              1\n",
      "2  Yellow             0              1              0\n",
      "3   Green             1              0              0\n",
      "4  Yellow             0              1              0\n",
      "5  Orange             0              0              1\n",
      "6  Yellow             0              1              0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fruits_data = pd.read_csv(\"fruits_dataset.csv\")\n",
    "\n",
    "df = pd.DataFrame(fruits_data)\n",
    "\n",
    "df = df.drop(columns=\"Price\")\n",
    "\n",
    "ohm = pd.get_dummies(df,columns=[\"Fruits\"],dtype=int)\n",
    "\n",
    "print(ohm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Fruits  Color_Green  Color_Orange  Color_Red  Color_Yellow\n",
      "0   Apple            0             0          1             0\n",
      "1  Orange            0             1          0             0\n",
      "2  Banana            0             0          0             1\n",
      "3   Apple            1             0          0             0\n",
      "4  Banana            0             0          0             1\n",
      "5  Orange            0             1          0             0\n",
      "6  Banana            0             0          0             1\n"
     ]
    }
   ],
   "source": [
    "ohm_color = pd.get_dummies(df,columns=[\"Color\"], dtype=int)\n",
    "print(ohm_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fruits</th>\n",
       "      <th>Color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple</td>\n",
       "      <td>Red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Orange</td>\n",
       "      <td>Orange</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Banana</td>\n",
       "      <td>Yellow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apple</td>\n",
       "      <td>Green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Banana</td>\n",
       "      <td>Yellow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fruits   Color\n",
       "0   Apple     Red\n",
       "1  Orange  Orange\n",
       "2  Banana  Yellow\n",
       "3   Apple   Green\n",
       "4  Banana  Yellow"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fruits_data = pd.read_csv(\"fruits_dataset.csv\")\n",
    "\n",
    "df = pd.DataFrame(fruits_data)\n",
    "\n",
    "df = df.drop(columns=\"Price\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['Fruits', 'Color'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ohm \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dummies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFruits\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mColor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(ohm)\n",
      "File \u001b[1;32mc:\\Users\\mistr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\encoding.py:169\u001b[0m, in \u001b[0;36mget_dummies\u001b[1;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput must be a list-like for parameter `columns`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m     data_to_encode \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# validate prefixes and separator to avoid silently dropping cols\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_len\u001b[39m(item, name: \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\mistr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4096\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4095\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4096\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4098\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\mistr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mistr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['Fruits', 'Color'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "ohm = pd.get_dummies(df,columns=[\"Fruits\",\"Color\"],dtype=int)\n",
    "\n",
    "print(ohm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot Encoding using Pandas Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fruits</th>\n",
       "      <th>Color</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple</td>\n",
       "      <td>Red</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Orange</td>\n",
       "      <td>Orange</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Banana</td>\n",
       "      <td>Yellow</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apple</td>\n",
       "      <td>Green</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Banana</td>\n",
       "      <td>Yellow</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fruits   Color  Price\n",
       "0   Apple     Red   0.75\n",
       "1  Orange  Orange   0.60\n",
       "2  Banana  Yellow   0.41\n",
       "3   Apple   Green   0.90\n",
       "4  Banana  Yellow   0.50"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fruits_data = pd.read_csv(\"fruits_dataset.csv\")\n",
    "\n",
    "df = pd.DataFrame(fruits_data)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=\"Price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fruits</th>\n",
       "      <th>Color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple</td>\n",
       "      <td>Red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Orange</td>\n",
       "      <td>Orange</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Banana</td>\n",
       "      <td>Yellow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apple</td>\n",
       "      <td>Green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Banana</td>\n",
       "      <td>Yellow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fruits   Color\n",
       "0   Apple     Red\n",
       "1  Orange  Orange\n",
       "2  Banana  Yellow\n",
       "3   Apple   Green\n",
       "4  Banana  Yellow"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Fruits  Color_Green  Color_Orange  Color_Red  Color_Yellow\n",
      "0   Apple            0             0          1             0\n",
      "1  Orange            0             1          0             0\n",
      "2  Banana            0             0          0             1\n",
      "3   Apple            1             0          0             0\n",
      "4  Banana            0             0          0             1\n",
      "5  Orange            0             1          0             0\n",
      "6  Banana            0             0          0             1\n"
     ]
    }
   ],
   "source": [
    "one_hot_encode = pd.get_dummies(df,columns=[\"Color\"],dtype=int)\n",
    "print(one_hot_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Fruits  Color_Green  Color_Orange  Color_Red  Color_Yellow\n",
      "0   Apple            0             0          1             0\n",
      "1  Orange            0             1          0             0\n",
      "2  Banana            0             0          0             1\n",
      "3   Apple            1             0          0             0\n",
      "4  Banana            0             0          0             1\n",
      "5  Orange            0             1          0             0\n",
      "6  Banana            0             0          0             1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fruits_data = pd.read_csv(\"fruits_dataset.csv\")\n",
    "\n",
    "df = pd.DataFrame(fruits_data)\n",
    "\n",
    "df = df.drop(columns=\"Price\")\n",
    "\n",
    "ohm = pd.get_dummies(df,columns=[\"Color\"],dtype=int)\n",
    "\n",
    "print(ohm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot Encoding using Sci-kit Learn Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "   Fruits   Color  Price\n",
      "0   Apple     Red   0.75\n",
      "1  Orange  Orange   0.60\n",
      "2  Banana  Yellow   0.41\n",
      "3   Apple   Green   0.90\n",
      "4  Banana  Yellow   0.50\n",
      "5  Orange  Orange   0.65\n",
      "6  Banana  Yellow   0.45\n",
      "\n",
      "Encoded Data:\n",
      "   Fruits_Apple  Fruits_Banana  Fruits_Orange\n",
      "0             1              0              0\n",
      "1             0              0              1\n",
      "2             0              1              0\n",
      "3             1              0              0\n",
      "4             0              1              0\n",
      "5             0              0              1\n",
      "6             0              1              0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "data = pd.read_csv(\"fruits_dataset.csv\")\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Fit and transform the 'Fruits' column\n",
    "ohm = encoder.fit_transform(data[['Fruits']])\n",
    "\n",
    "# Create a DataFrame from the one-hot encoded data\n",
    "one_hot = pd.DataFrame(ohm.toarray(), columns=encoder.get_feature_names_out(['Fruits']), dtype=int)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "\n",
    "print(\"\\nEncoded Data:\")\n",
    "print(one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Fruits  Color_Green  Color_Orange  Color_Red  Color_Yellow\n",
      "0   Apple            0             0          1             0\n",
      "1  Orange            0             1          0             0\n",
      "2  Banana            0             0          0             1\n",
      "3   Apple            1             0          0             0\n",
      "4  Banana            0             0          0             1\n",
      "5  Orange            0             1          0             0\n",
      "6  Banana            0             0          0             1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "data = pd.read_csv(\"fruits_dataset.csv\")\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "one_hot_encode = encoder.fit_transform(data[[\"Color\"]])\n",
    "\n",
    "one_hot_df = pd.DataFrame(one_hot_encode.toarray(),columns=encoder.get_feature_names_out([\"Color\"]), dtype=int)\n",
    "\n",
    "result = pd.concat([data[\"Fruits\"], one_hot_df],axis=1)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Fruits Data\n",
      "   Fruits   Color  Price\n",
      "0   Apple     Red   0.75\n",
      "1  Orange  Orange   0.60\n",
      "2  Banana  Yellow   0.41\n",
      "3   Apple   Green   0.90\n",
      "4  Banana  Yellow   0.50\n",
      "5  Orange  Orange   0.65\n",
      "6  Banana  Yellow   0.45\n",
      "\n",
      "Encoded Fruits Data\n",
      "   Fruits  Color_Green  Color_Orange  Color_Red  Color_Yellow\n",
      "0   Apple            0             0          1             0\n",
      "1  Orange            0             1          0             0\n",
      "2  Banana            0             0          0             1\n",
      "3   Apple            1             0          0             0\n",
      "4  Banana            0             0          0             1\n",
      "5  Orange            0             1          0             0\n",
      "6  Banana            0             0          0             1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "data = pd.read_csv(\"fruits_dataset.csv\")\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "ohm = encoder.fit_transform(data[[\"Color\"]])\n",
    "\n",
    "ohm_df = pd.DataFrame(ohm.toarray(), columns=encoder.get_feature_names_out([\"Color\"]), dtype=int)\n",
    "\n",
    "result = pd.concat([data[\"Fruits\"], ohm_df], axis=1)\n",
    "\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original Fruits Data\")\n",
    "print(data)\n",
    "\n",
    "print(\"\\nEncoded Fruits Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Fruits  Color_Green  Color_Orange  Color_Red  Color_Yellow\n",
      "0   Apple            0             0          1             0\n",
      "1  Orange            0             1          0             0\n",
      "2  Banana            0             0          0             1\n",
      "3   Apple            1             0          0             0\n",
      "4  Banana            0             0          0             1\n",
      "5  Orange            0             1          0             0\n",
      "6  Banana            0             0          0             1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load the fruits dataset from the CSV file\n",
    "data = pd.read_csv(\"fruits_dataset.csv\")\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Fit and transform the 'Color' column\n",
    "ohm = encoder.fit_transform(data[[\"Color\"]])\n",
    "\n",
    "# Create a DataFrame from the one-hot encoded data\n",
    "ohm_df = pd.DataFrame(ohm.toarray(), columns=encoder.get_feature_names_out([\"Color\"]), dtype=int)\n",
    "\n",
    "# Concatenate the 'Fruits' column with the one-hot encoded DataFrame\n",
    "result = pd.concat([data[\"Fruits\"], ohm_df], axis=1)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document\n",
      "['The quick brown fox jumps over the lazy dog.', 'The lazy dog sleeps peacefully.', 'The quick brown fox and the lazy dog are friends.']\n",
      "\n",
      "Count Vectors:\n",
      "[[0 0 1 1 1 0 1 1 1 0 1 0 2]\n",
      " [0 0 0 1 0 0 0 1 0 1 0 1 1]\n",
      " [1 1 1 1 1 1 0 1 0 0 1 0 2]]\n",
      "\n",
      "Vocabulary:\n",
      "['and' 'are' 'brown' 'dog' 'fox' 'friends' 'jumps' 'lazy' 'over'\n",
      " 'peacefully' 'quick' 'sleeps' 'the']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Step 1: Sample documents\n",
    "documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The lazy dog sleeps peacefully.\",\n",
    "    \"The quick brown fox and the lazy dog are friends.\"\n",
    "]\n",
    "\n",
    "# Step 2: Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Step 3: Tokenization and vocabulary creation\n",
    "# Fit the CountVectorizer to learn the vocabulary and transform documents into count vectors\n",
    "count_vectors = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Step 4: Access the vocabulary\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Step 5: Print count vectors and vocabulary\n",
    "print(\"Document\")\n",
    "print(documents)\n",
    "print(\"\\nCount Vectors:\")\n",
    "print(count_vectors.toarray())\n",
    "print(\"\\nVocabulary:\")\n",
    "print(vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "text = \"\"\"Beans. I was trying to explain to somebody as we were flying in, thats corn. Thats beans. And they were very impressed at my agricultural knowledge. Please give it up for Amaury once again for that outstanding introduction. I have a bunch of good friends here today, including somebody who I served with, who is one of the finest senators in the country, and were lucky to have him, your Senator, Dick Durbin is here. I also noticed, by the way, former Governor Edgar here, who I havent seen in a long time, and somehow he has not aged and I have. And its great to see you, Governor. I want to thank President Killeen and everybody at the U of I System for making it possible for me to be here today. And I am deeply honored at the Paul Douglas Award that is being given to me. He is somebody who set the path for so much outstanding public service here in Illinois. Now, I want to start by addressing the elephant in the room. I know people are still wondering why I didnt speak at the commencement.\"\"\"\n",
    "\n",
    "dataset = nltk.sent_tokenize(text)\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "  dataset[i] = dataset[i].lower()\n",
    "  dataset[i] = re.sub(r\"\\w\",\" \",dataset[i])\n",
    "  dataset[i] = re.sub(r\"\\s+\", \" \",dataset[i]) \n",
    "\n",
    "\n",
    "word2count = {} \n",
    "for data in dataset: \n",
    "    words = nltk.word_tokenize(data) \n",
    "    for word in words: \n",
    "        if word not in word2count.keys(): \n",
    "            word2count[word] = 1\n",
    "        else: \n",
    "            word2count[word] += 1\n",
    "\n",
    "import heapq \n",
    "freq_words = heapq.nlargest(100, word2count, key=word2count.get)\n",
    "\n",
    "X = [] \n",
    "for data in dataset: \n",
    "    vector = [] \n",
    "    for word in freq_words: \n",
    "        if word in nltk.word_tokenize(data): \n",
    "            vector.append(1) \n",
    "        else: \n",
    "            vector.append(0) \n",
    "    X.append(vector) \n",
    "X = np.asarray(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectors:\n",
      "[[0 0 1 1 1 0 1 1 1 0 1 0 2]\n",
      " [0 0 0 1 0 0 0 1 0 1 0 1 1]\n",
      " [1 1 1 1 1 1 0 1 0 0 1 0 2]]\n",
      "\n",
      "Vocabulary:\n",
      "['and' 'are' 'brown' 'dog' 'fox' 'friends' 'jumps' 'lazy' 'over'\n",
      " 'peacefully' 'quick' 'sleeps' 'the']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The lazy dog sleeps peacefully.\",\n",
    "    \"The quick brown fox and the lazy dog are friends.\"\n",
    "]\n",
    "\n",
    "# Step 1: Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Step 2: Learn the vocabulary and transform documents into count vectors\n",
    "count_vectors = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Step 3: Access the vocabulary\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Step 4: Print the count vectors and vocabulary\n",
    "print(\"Count Vectors:\")\n",
    "print(count_vectors.toarray())\n",
    "print(\"\\nVocabulary:\")\n",
    "print(vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'cat', 'is', 'on', 'the', 'mat', '!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the necessary resources for word tokenization\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"The cat is on the mat!\"\n",
    "\n",
    "# Tokenize the sentence into words\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word   Index\n",
      "!:   0\n",
      ".:   1\n",
      "The:   2\n",
      "cat:   3\n",
      "dog:   4\n",
      "is:   5\n",
      "mat:   6\n",
      "on:   7\n",
      "table:   8\n",
      "the:   9\n",
      "under:   10\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The cat is on the mat!', 'The dog is under the table.']\n",
      "Word Count Vectors:\n",
      "Document 1: [1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0]\n",
      "Document 2: [0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "def create_word_count_vectors(corpus, vocabulary, word_to_index):\n",
    "    word_count_vectors = []\n",
    "    # Iterate through each document in the corpus\n",
    "    for document in corpus:\n",
    "        # Initialize a vector with zeros for each word in the vocabulary\n",
    "        word_count_vector = [0] * len(vocabulary)\n",
    "        # Tokenize the document into words\n",
    "        tokens = word_tokenize(document)\n",
    "        # Count the occurrence of each word\n",
    "        for token in tokens:\n",
    "            if token in vocabulary:\n",
    "                word_index = word_to_index[token]\n",
    "                word_count_vector[word_index] += 1\n",
    "        # Add the word count vector to the list\n",
    "        word_count_vectors.append(word_count_vector)\n",
    "    return word_count_vectors\n",
    "\n",
    "# Example corpus\n",
    "corpus = [\n",
    "    \"The cat is on the mat!\",\n",
    "    \"The dog is under the table.\"\n",
    "]\n",
    "\n",
    "# Create the vocabulary\n",
    "vocabulary = create_vocabulary(corpus)\n",
    "# Create a word-to-index mapping\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "# Create word count vectors for each document\n",
    "word_count_vectors = create_word_count_vectors(corpus, vocabulary, word_to_index)\n",
    "\n",
    "# Print the word count vectors\n",
    "print(corpus)\n",
    "print(\"Word Count Vectors:\")\n",
    "for i, vector in enumerate(word_count_vectors):\n",
    "    print(f\"Document {i + 1}: {vector}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Matrix:\n",
      "[[0. 2. 1. 1. 1. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Download NLTK resources if not already downloaded\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Sample documents\n",
    "documents = [\"The quick brown fox jumps over the lazy dog.\", \"The lazy dog sleeps peacefully.\"]\n",
    "\n",
    "# Tokenization and remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "bag_of_words = []\n",
    "\n",
    "for doc in documents:\n",
    "    tokens = word_tokenize(doc)\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    word_freq = Counter(filtered_tokens)\n",
    "    bag_of_words.append(word_freq)\n",
    "\n",
    "# Convert bag_of_words into a matrix format\n",
    "# This matrix represents the bag-of-words representation\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(documents)\n",
    "bag_of_words_matrix = tokenizer.texts_to_matrix(documents, mode='count')\n",
    "\n",
    "print(\"Bag of Words Matrix:\")\n",
    "print(bag_of_words_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus:\n",
      "['The cat is on the mat!', 'The dog is under the table.']\n",
      "Word Count Vectors:\n",
      "Document 1: [0, 2, 1, 1, 1, 1, 0, 0, 0]\n",
      "Document 2: [0, 2, 1, 0, 0, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "# Download NLTK resources if not already downloaded\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "def create_word_count_vectors(corpus, tokenizer):\n",
    "    word_count_vectors = []\n",
    "    # Convert the corpus into sequences of word indices\n",
    "    sequences = tokenizer.texts_to_sequences(corpus)\n",
    "    # Iterate through each sequence\n",
    "    for seq in sequences:\n",
    "        # Initialize a vector with zeros for each unique word index\n",
    "        word_count_vector = [0] * (len(tokenizer.word_index) + 1)\n",
    "        # Count the occurrence of each word index\n",
    "        for idx in seq:\n",
    "            word_count_vector[idx] += 1\n",
    "        # Add the word count vector to the list\n",
    "        word_count_vectors.append(word_count_vector)\n",
    "    return word_count_vectors\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"The cat is on the mat!\",\n",
    "    \"The dog is under the table.\"\n",
    "]\n",
    "\n",
    "# Initialize the Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "# Create word count vectors for each document\n",
    "word_count_vectors = create_word_count_vectors(corpus, tokenizer)\n",
    "\n",
    "# Print the word count vectors\n",
    "print(\"Corpus:\")\n",
    "print(corpus)\n",
    "print(\"Word Count Vectors:\")\n",
    "for i, vector in enumerate(word_count_vectors):\n",
    "    print(f\"Document {i + 1}: {vector}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'is', 'cat', 'sleeping', 'on', 'mat', 'dog', 'running', 'in', 'garden']\n",
      "[[0. 2. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 2. 1. 0. 0. 0. 0. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "docs = [\n",
    "  'The cat is sleeping on the mat.',\n",
    "  'The dog is running in the garden.'\n",
    "]\n",
    "\n",
    "## Step 1: Determine the Vocabulary\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(docs)\n",
    "print(f'{list(tokenizer.word_index.keys())}')\n",
    "\n",
    "## Step 2: Count\n",
    "vectors = tokenizer.texts_to_matrix(docs, mode='count')\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words without stopwords:\n",
      "['cat', 'sleeping', 'mat', '.', 'dog', 'running', 'garden', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the stopwords corpus (only needed once)\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Define the documents\n",
    "documents = [\n",
    "    \"The cat is sleeping on the mat.\",\n",
    "    \"The dog is running in the garden.\",\n",
    "    \n",
    "]\n",
    "\n",
    "# Get the list of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize each document and remove stopwords\n",
    "words_without_stopwords = []\n",
    "for document in documents:\n",
    "    # Tokenize the document into words\n",
    "    words = word_tokenize(document)\n",
    "    # Remove stopwords from the tokenized words\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    # Add the filtered words to the list\n",
    "    words_without_stopwords.extend(filtered_words)\n",
    "\n",
    "# Display the remaining words after removing stopwords\n",
    "print(\"Words without stopwords:\")\n",
    "print(words_without_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Count Vectors:\n",
      "['cat sleeping mat', 'dog running garden']\n",
      "[[1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Define the documents\n",
    "documents = [\n",
    "    \"The cat is sleeping on the mat\", \n",
    "    \"The dog is running in the garden\"\n",
    "]\n",
    "\n",
    "# Tokenize each document\n",
    "word_tokens = [word_tokenize(document) for document in documents]\n",
    "\n",
    "# Get the list of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stopwords from each document\n",
    "filtered_word_tokens = [[word for word in tokens if word.lower() not in stop_words] for tokens in word_tokens]\n",
    "\n",
    "# Combine the filtered word tokens into a single row\n",
    "combined_tokens = [' '.join(tokens) for tokens in filtered_word_tokens]\n",
    "\n",
    "# Create a Tokenizer instance and fit it on the combined tokens\n",
    "tokenizer = Tokenizer(num_words=None, lower=True, split=' ', char_level=False, oov_token=None)\n",
    "tokenizer.fit_on_texts(combined_tokens)\n",
    "\n",
    "# Convert combined tokens to sequences of word indices\n",
    "word_index_sequences = tokenizer.texts_to_sequences(combined_tokens)\n",
    "\n",
    "# Create Word Count Vectors\n",
    "word_count_vectors = tokenizer.sequences_to_matrix(word_index_sequences, mode='count')\n",
    "\n",
    "# Adjust indices to start from 1\n",
    "word_count_vectors_shifted = word_count_vectors[:, 1:]\n",
    "\n",
    "# Print the word count vectors\n",
    "print(\"Word Count Vectors:\")\n",
    "print(combined_tokens)\n",
    "print(word_count_vectors_shifted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Count Vectors:\n",
      "['cat sleeping mat', 'dog running garden']\n",
      "[[1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Define the documents\n",
    "documents = [\n",
    "    \"The cat is sleeping on the mat\", \n",
    "    \"The dog is running in the garden\"\n",
    "]\n",
    "\n",
    "# Tokenize each document\n",
    "word_tokens = [word_tokenize(document) for document in documents]\n",
    "\n",
    "# Get the list of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stopwords from each document\n",
    "filtered_word_tokens = [[word for word in tokens if word.lower() not in stop_words] for tokens in word_tokens]\n",
    "\n",
    "# Combine the filtered word tokens into a single row\n",
    "combined_tokens = [' '.join(tokens) for tokens in filtered_word_tokens]\n",
    "\n",
    "# Create a Tokenizer instance and fit it on the combined tokens\n",
    "tokenizer = Tokenizer(num_words=None, lower=True, split=' ', char_level=False, oov_token=None)\n",
    "tokenizer.fit_on_texts(combined_tokens)\n",
    "\n",
    "# Convert combined tokens to sequences of word indices\n",
    "word_index_sequences = tokenizer.texts_to_sequences(combined_tokens)\n",
    "\n",
    "# Create Word Count Vectors\n",
    "word_count_vectors = tokenizer.sequences_to_matrix(word_index_sequences, mode='count')\n",
    "\n",
    "# Adjust indices to start from 1\n",
    "word_count_vectors_shifted = word_count_vectors[:, 1:]\n",
    "\n",
    "# Print the word count vectors\n",
    "print(\"Word Count Vectors:\")\n",
    "print(combined_tokens)\n",
    "print(word_count_vectors_shifted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 1, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "documents = [\n",
    "    \"The cat is sleeping on the mat\", \n",
    "    \"The dog is running in the garden\"\n",
    "]\n",
    "\n",
    "bag_of_words = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "bag_of_words.fit(documents)\n",
    "\n",
    "bag_of_words.get_feature_names_out()\n",
    "\n",
    "bow_features = bag_of_words.transform(documents)\n",
    "\n",
    "bow_features_array = bow_features.toarray()\n",
    "bow_features_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat' 'dog' 'garden' 'mat' 'running' 'sleeping']\n",
      "The cat is sleeping on the mat\n",
      "[1 0 0 1 0 1]\n",
      "The dog is running in the garden\n",
      "[0 1 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(bag_of_words.get_feature_names_out())\n",
    "for sentence,feature in zip(documents,bow_features_array):\n",
    "  print(sentence)\n",
    "  print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat' 'dog' 'garden' 'mat' 'running' 'sleeping']\n",
      "The cat is sleeping on the mat\n",
      "[1 0 0 1 0 1]\n",
      "The dog is running in the garden\n",
      "[0 1 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "documents = [\n",
    "    \"The cat is sleeping on the mat\", \n",
    "    \"The dog is running in the garden\"\n",
    "]\n",
    "\n",
    "bag_of_words = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "bag_of_words.fit(documents)\n",
    "\n",
    "bag_of_words.get_feature_names_out()\n",
    "\n",
    "bow_features = bag_of_words.transform(documents)\n",
    "\n",
    "bow_features_array = bow_features.toarray()\n",
    "bow_features_array\n",
    "\n",
    "print(bag_of_words.get_feature_names_out())\n",
    "for sentence,feature in zip(documents,bow_features_array):\n",
    "  print(sentence)\n",
    "  print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Count Vectors using Tokenizer:\n",
      "['cat sleeping mat', 'dog running garden']\n",
      "[[1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 1.]]\n",
      "\n",
      "Word Count Vectors using CountVectorizer:\n",
      "['cat' 'dog' 'garden' 'mat' 'running' 'sleeping']\n",
      "The cat is sleeping on the mat\n",
      "[1 0 0 1 0 1]\n",
      "The dog is running in the garden\n",
      "[0 1 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define the documents\n",
    "documents = [\n",
    "    \"The cat is sleeping on the mat\", \n",
    "    \"The dog is running in the garden\"\n",
    "]\n",
    "\n",
    "# Tokenize each document\n",
    "word_tokens = [word_tokenize(document) for document in documents]\n",
    "\n",
    "# Get the list of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stopwords from each document\n",
    "filtered_word_tokens = [[word for word in tokens if word.lower() not in stop_words] for tokens in word_tokens]\n",
    "\n",
    "# Combine the filtered word tokens into a single row\n",
    "combined_tokens = [' '.join(tokens) for tokens in filtered_word_tokens]\n",
    "\n",
    "# Create a Tokenizer instance and fit it on the combined tokens\n",
    "tokenizer = Tokenizer(num_words=None, lower=True, split=' ', char_level=False, oov_token=None)\n",
    "tokenizer.fit_on_texts(combined_tokens)\n",
    "\n",
    "# Convert combined tokens to sequences of word indices\n",
    "word_index_sequences = tokenizer.texts_to_sequences(combined_tokens)\n",
    "\n",
    "# Create Word Count Vectors using the Tokenizer\n",
    "word_count_vectors = tokenizer.sequences_to_matrix(word_index_sequences, mode='count')\n",
    "\n",
    "# Adjust indices to start from 1\n",
    "word_count_vectors_shifted = word_count_vectors[:, 1:]\n",
    "\n",
    "# Print the word count vectors using Tokenizer\n",
    "print(\"Word Count Vectors using Tokenizer:\")\n",
    "print(combined_tokens)\n",
    "print(word_count_vectors_shifted)\n",
    "\n",
    "# Create Word Count Vectors using CountVectorizer\n",
    "bag_of_words = CountVectorizer(stop_words=\"english\")\n",
    "bow_features = bag_of_words.fit_transform(combined_tokens)\n",
    "bow_features_array = bow_features.toarray()\n",
    "\n",
    "# Print the word count vectors using CountVectorizer\n",
    "print(\"\\nWord Count Vectors using CountVectorizer:\")\n",
    "print(bag_of_words.get_feature_names_out())\n",
    "for sentence, feature in zip(documents, bow_features_array):\n",
    "    print(sentence)\n",
    "    print(feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'dog', 'garden', 'mat', 'running', 'sleeping']\n",
      "The cat is sleeping on the mat\n",
      "[1, 0, 0, 1, 0, 1]\n",
      "The dog is running in the garden\n",
      "[0, 1, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "documents = [\n",
    "    \"The cat is sleeping on the mat\", \n",
    "    \"The dog is running in the garden\"\n",
    "]\n",
    "\n",
    "bag_of_words = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "bag_of_words.fit(documents)\n",
    "\n",
    "feature_names = bag_of_words.get_feature_names_out()\n",
    "\n",
    "bow_features = bag_of_words.transform(documents)\n",
    "\n",
    "bow_features_array = bow_features.toarray()\n",
    "\n",
    "desired_features_order = ['cat', 'dog', 'garden', 'mat', 'running', 'sleeping']\n",
    "\n",
    "print(desired_features_order)\n",
    "\n",
    "for sentence, feature in zip(documents, bow_features_array):\n",
    "    print(sentence)\n",
    "    print([feature[np.where(feature_names == word)[0][0]] if word in feature_names else 0 for word in desired_features_order])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'sleeping', 'mat', 'dog', 'running', 'garden']\n",
      "['The cat is sleeping on the mat\\n', 'The dog is running in the garden']\n",
      "The cat is sleeping on the mat\n",
      "\n",
      "[1. 1. 1. 0. 0. 0.]\n",
      "The dog is running in the garden\n",
      "[0. 0. 0. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Define the documents\n",
    "documents = [\n",
    "    \"The cat is sleeping on the mat\", \n",
    "    \"The dog is running in the garden\"\n",
    "]\n",
    "\n",
    "# Tokenize each document\n",
    "word_tokens = [word_tokenize(document) for document in documents]\n",
    "\n",
    "# Get the list of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stopwords from each document\n",
    "filtered_word_tokens = [[word for word in tokens if word.lower() not in stop_words] for tokens in word_tokens]\n",
    "\n",
    "# Create a Tokenizer instance and fit it on the filtered tokens\n",
    "tokenizer = Tokenizer(num_words=None, lower=True, split=' ', char_level=False, oov_token=None)\n",
    "tokenizer.fit_on_texts(filtered_word_tokens)\n",
    "\n",
    "# Convert filtered tokens to sequences of word indices\n",
    "word_index_sequences = tokenizer.texts_to_sequences(filtered_word_tokens)\n",
    "\n",
    "# Create Word Count Vectors\n",
    "word_count_vectors = tokenizer.sequences_to_matrix(word_index_sequences, mode='count')\n",
    "\n",
    "# Adjust indices to start from 1\n",
    "word_count_vectors_shifted = word_count_vectors[:, 1:]\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = tokenizer.index_word.values()\n",
    "\n",
    "# Print the feature names\n",
    "print(list(feature_names))\n",
    "\n",
    "# Print the word count vectors\n",
    "print(documents)\n",
    "for document, vector in zip(documents, word_count_vectors_shifted):\n",
    "    print(document)\n",
    "    print(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'sleeping', 'mat', 'dog', 'running', 'garden']\n",
      "[1. 1. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Define the documents\n",
    "document1 = \"The cat is sleeping on the mat\"\n",
    "document2 = \"The dog is running in the garden\"\n",
    "\n",
    "# Combine the documents into a list\n",
    "documents = [document1, document2]\n",
    "\n",
    "# Tokenize each document\n",
    "word_tokens = [word_tokenize(document) for document in documents]\n",
    "\n",
    "# Get the list of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stopwords from each document\n",
    "filtered_word_tokens = [[word for word in tokens if word.lower() not in stop_words] for tokens in word_tokens]\n",
    "\n",
    "# Create a Tokenizer instance and fit it on the filtered tokens\n",
    "tokenizer = Tokenizer(num_words=None, lower=True, split=' ', char_level=False, oov_token=None)\n",
    "tokenizer.fit_on_texts(filtered_word_tokens)\n",
    "\n",
    "# Convert filtered tokens to sequences of word indices\n",
    "word_index_sequences = tokenizer.texts_to_sequences(filtered_word_tokens)\n",
    "\n",
    "# Create Word Count Vectors\n",
    "word_count_vectors = tokenizer.sequences_to_matrix(word_index_sequences, mode='count')\n",
    "\n",
    "# Adjust indices to start from 1\n",
    "word_count_vectors_shifted = word_count_vectors[:, 1:]\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = tokenizer.index_word.values()\n",
    "\n",
    "# Print the feature names\n",
    "print(list(feature_names))\n",
    "\n",
    "# Print the word count vectors\n",
    "# print(documents)\n",
    "for document, vector in zip(documents, word_count_vectors_shifted):\n",
    "    print(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['the', 'cat', 'sat', 'hat', 'in', 'with']\n",
      "[[0. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 2. 1. 1. 1. 1. 0.]\n",
      " [0. 2. 1. 0. 1. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "document1 = \"The cat is sleeping on the mat\"\n",
    "document2 = \"The dog is running in the garden\"\n",
    "\n",
    "## Step 1: Determine the Vocabulary\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(docs)\n",
    "print(f'Vocabulary: {list(tokenizer.word_index.keys())}')\n",
    "\n",
    "## Step 2: Count\n",
    "vectors = tokenizer.texts_to_matrix(docs, mode='count')\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'is', 'cat', 'sleeping', 'on', 'mat', 'dog', 'running', 'in', 'garden']\n",
      "The cat is sleeping on the mat\n",
      "[[0. 2. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 2. 1. 0. 0. 0. 0. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Define the documents\n",
    "document1 = \"The cat is sleeping on the mat\"\n",
    "document2 = \"The dog is running in the garden\"\n",
    "\n",
    "# Step 1: Determine the Vocabulary\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([document1, document2])\n",
    "print(f'{list(tokenizer.word_index.keys())}')\n",
    "\n",
    "# Step 2: Count\n",
    "vectors = tokenizer.texts_to_matrix([document1, document2], mode='count')\n",
    "print(document1)\n",
    "print(vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['the', 'is', 'cat', 'sleeping', 'on', 'mat', 'dog', 'running', 'in', 'garden']\n",
      "The dog is running in the garden\n",
      "[[0. 2. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 2. 1. 0. 0. 0. 0. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Define the documents\n",
    "documents = [\n",
    "    \"The cat is sleeping on the mat\", \n",
    "    \"The dog is running in the garden\"\n",
    "]\n",
    "\n",
    "# Step 1: Determine the Vocabulary\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(documents)\n",
    "print(f'Vocabulary: {list(tokenizer.word_index.keys())}')\n",
    "\n",
    "# Step 2: Count\n",
    "vectors = tokenizer.texts_to_matrix(documents, mode='count')\n",
    "print(document)\n",
    "print(vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bag of Words using scikit-learn CountVectorizer (with default parameters):\n",
      "   amazing  an  best  game  great  is  of  series  so  the  thrones  tv\n",
      "0        1   1     0     1      0   1   1       1   0    0        1   1\n",
      "1        0   0     1     1      0   1   1       1   0    1        1   1\n",
      "2        0   0     0     1      1   1   1       0   1    0        1   0\n",
      "\n",
      "Bag of Words using scikit-learn CountVectorizer with stop words removal:\n",
      "   amazing  best  game  great  series  thrones  tv\n",
      "0        1     0     1      0       1        1   1\n",
      "1        0     1     1      0       1        1   1\n",
      "2        0     0     1      1       0        1   0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define the documents\n",
    "doc1 = 'Game of Thrones is an amazing tv series!'\n",
    "doc2 = 'Game of Thrones is the best tv series!'\n",
    "doc3 = 'Game of Thrones is so great'\n",
    "\n",
    "# Define a function to calculate Bag of Words (BOW) using custom Python code\n",
    "def calculateBOW(wordset, l_doc):\n",
    "    tf_diz = dict.fromkeys(wordset, 0)\n",
    "    for word in l_doc:\n",
    "        tf_diz[word] = l_doc.count(word)\n",
    "    return tf_diz\n",
    "\n",
    "# Preprocess the documents and split into word lists\n",
    "l_doc1 = re.sub(r\"[^a-zA-Z0-9]\", \" \", doc1.lower()).split()\n",
    "l_doc2 = re.sub(r\"[^a-zA-Z0-9]\", \" \", doc2.lower()).split()\n",
    "l_doc3 = re.sub(r\"[^a-zA-Z0-9]\", \" \", doc3.lower()).split()\n",
    "\n",
    "# Create a set of unique words from all documents\n",
    "wordset = set(l_doc1).union(set(l_doc2)).union(set(l_doc3))\n",
    "\n",
    "# Calculate BOW for each document using custom Python code\n",
    "bow1 = calculateBOW(wordset, l_doc1)\n",
    "bow2 = calculateBOW(wordset, l_doc2)\n",
    "bow3 = calculateBOW(wordset, l_doc3)\n",
    "\n",
    "# Create DataFrame to display BOW representations calculated using custom Python code\n",
    "# df_bow = pd.DataFrame([bow1, bow2, bow3])\n",
    "# print(\"Bag of Words using custom Python code:\")\n",
    "# print(df_bow)\n",
    "\n",
    "# Use CountVectorizer from scikit-learn to calculate BOW representations\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([doc1, doc2, doc3])\n",
    "df_bow_sklearn = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(\"\\nBag of Words using scikit-learn CountVectorizer (with default parameters):\")\n",
    "print(df_bow_sklearn)\n",
    "\n",
    "# Use CountVectorizer with stop words removal\n",
    "vectorizer_sw = CountVectorizer(stop_words='english')\n",
    "X_sw = vectorizer_sw.fit_transform([doc1, doc2, doc3])\n",
    "df_bow_sklearn_sw = pd.DataFrame(X_sw.toarray(), columns=vectorizer_sw.get_feature_names_out())\n",
    "print(\"\\nBag of Words using scikit-learn CountVectorizer with stop words removal:\")\n",
    "print(df_bow_sklearn_sw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bag of Words using scikit-learn CountVectorizer with stop words removal: \n",
      "\n",
      "   cat  dog  garden  mat  running  sleeping\n",
      "0    1    0       0    1        0         1\n",
      "1    0    1       1    0        1         0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define the documents\n",
    "doc1 = 'The cat is sleeping on the mat'\n",
    "doc2 = 'The dog is running in the garden'\n",
    "\n",
    "# Use CountVectorizer from scikit-learn to calculate BOW representations\n",
    "# vectorizer = CountVectorizer()\n",
    "# X = vectorizer.fit_transform([doc1, doc2])\n",
    "# df_bow_sklearn = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "# print(\"\\nBag of Words using scikit-learn CountVectorizer (with default parameters):\")\n",
    "# print(document1)\n",
    "# print(df_bow_sklearn)\n",
    "\n",
    "# Use CountVectorizer with stop words removal\n",
    "vectorizer_sw = CountVectorizer(stop_words='english')\n",
    "X_sw = vectorizer_sw.fit_transform([doc1, doc2])\n",
    "df_bow_sklearn_sw = pd.DataFrame(X_sw.toarray(), columns=vectorizer_sw.get_feature_names_out())\n",
    "print(\"\\nBag of Words using scikit-learn CountVectorizer with stop words removal: \")\n",
    "print()\n",
    "print(df_bow_sklearn_sw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary with stop words removed:\n",
      "['cat' 'dog' 'garden' 'mat' 'running' 'sleeping']\n",
      "\n",
      "Bag of Words using scikit-learn CountVectorizer with stop words removal:\n",
      "   cat  dog  garden  mat  running  sleeping\n",
      "0    1    0       0    1        0         1\n",
      "1    0    1       1    0        1         0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define the documents\n",
    "doc1 = 'The cat is sleeping on the mat'\n",
    "doc2 = 'The dog is running in the garden'\n",
    "\n",
    "# Display the vocabulary with stop words removed\n",
    "print(\"\\nVocabulary with stop words removed:\")\n",
    "print(vectorizer_sw.get_feature_names_out())\n",
    "\n",
    "# Use CountVectorizer with stop words removal\n",
    "vectorizer_sw = CountVectorizer(stop_words='english')\n",
    "X_sw = vectorizer_sw.fit_transform([doc1, doc2])\n",
    "df_bow_sklearn_sw = pd.DataFrame(X_sw.toarray(), columns=vectorizer_sw.get_feature_names_out())\n",
    "print(\"\\nBag of Words using scikit-learn CountVectorizer with stop words removal:\")\n",
    "print(df_bow_sklearn_sw)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "['cat' 'dog' 'garden' 'in' 'is' 'mat' 'on' 'running' 'sleeping' 'the']\n",
      "      cat  dog  garden  in  is  mat  on  running  sleeping  the\n",
      "Doc1    1    0       0   0   1    1   1        0         1    2\n",
      "Doc2    0    1       1   1   1    0   0        1         0    2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define the documents\n",
    "doc1 = \"The cat is sleeping on the mat.\"\n",
    "doc2 = \"The dog is running in the garden.\"\n",
    "\n",
    "# Create a CountVectorizer instance\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the documents and transform them into a bag of words representation\n",
    "bag_of_words = vectorizer.fit_transform([doc1, doc2])\n",
    "\n",
    "# Convert the bag of words representation to a DataFrame for better visualization\n",
    "import pandas as pd\n",
    "bow_df = pd.DataFrame(bag_of_words.toarray(), columns=vectorizer.get_feature_names_out(), index=['Doc1', 'Doc2'])\n",
    "\n",
    "# Print the vocabulary\n",
    "print(\"Vocabulary:\")\n",
    "# print(doc1)\n",
    "# print(doc2)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Print the bag of words representation\n",
    "# print(\"\\nBag of Words Representation:\")\n",
    "print(bow_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  ['cat' 'dog' 'garden' 'mat' 'running' 'sleeping']\n",
      "\n",
      "\n",
      "Bag of Words Representation after removing stop words:\n",
      "\n",
      "      cat  dog  garden  mat  running  sleeping\n",
      "Doc1    1    0       0    1        0         1\n",
      "Doc2    0    1       1    0        1         0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Define the documents\n",
    "doc1 = \"The cat is sleeping on the mat.\"\n",
    "doc2 = \"The dog is running in the garden.\"\n",
    "\n",
    "# Create a CountVectorizer instance with stop words removed\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit the vectorizer on the documents and transform them into a bag of words representation\n",
    "bag_of_words = vectorizer.fit_transform([doc1, doc2])\n",
    "\n",
    "# Convert the bag of words representation to a DataFrame for better visualization\n",
    "\n",
    "bow_df = pd.DataFrame(bag_of_words.toarray(), columns=vectorizer.get_feature_names_out(), index=['Doc1', 'Doc2'])\n",
    "\n",
    "# Print the vocabulary\n",
    "print(\"Vocabulary: \",vectorizer.get_feature_names_out())\n",
    "print()\n",
    "\n",
    "# Print the bag of words representation after removing stop words\n",
    "print(\"\\nBag of Words Representation after removing stop words:\\n\")\n",
    "print(bow_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  ['cat' 'dog' 'garden' 'mat' 'running' 'sleeping']\n",
      "\n",
      "Bag of Words Representation after removing stop words:\n",
      "\n",
      "      cat  dog  garden  mat  running  sleeping\n",
      "Doc1    1    0       0    1        0         1\n",
      "Doc2    0    1       1    0        1         0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Define the documents\n",
    "doc1 = \"The cat is sleeping on the mat.\"\n",
    "doc2 = \"The dog is running in the garden.\"\n",
    "\n",
    "# Create a CountVectorizer instance with stop words removed\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit the vectorizer on the documents and transform them into a bag of words representation\n",
    "bag_of_words = vectorizer.fit_transform([doc1, doc2])\n",
    "\n",
    "# Convert the bag of words representation to a DataFrame for better visualization\n",
    "bow_df = pd.DataFrame(bag_of_words.toarray(), columns=vectorizer.get_feature_names_out(), index=['Doc1', 'Doc2'])\n",
    "\n",
    "# Print the vocabulary\n",
    "print(\"Vocabulary: \", vectorizer.get_feature_names_out())\n",
    "print()\n",
    "\n",
    "# Print the bag of words representation after removing stop words\n",
    "print(\"Bag of Words Representation after removing stop words:\\n\")\n",
    "print(bow_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "    cat: 0.341\n",
      "    mat: 0.448\n",
      "    on: 0.448\n",
      "    sat: 0.448\n",
      "    the: 0.530\n",
      "Document 2:\n",
      "    dog: 0.341\n",
      "    in: 0.448\n",
      "    played: 0.448\n",
      "    the: 0.530\n",
      "    yard: 0.448\n",
      "Document 3:\n",
      "    and: 0.424\n",
      "    are: 0.424\n",
      "    cat: 0.323\n",
      "    dog: 0.323\n",
      "    friends: 0.424\n",
      "    the: 0.501\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Example documents\n",
    "documents = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog played in the yard.\",\n",
    "    \"The cat and the dog are friends.\"\n",
    "]\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the documents and transform them to TF-IDF matrix\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names (terms)\n",
    "terms = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print TF-IDF scores for each term in each document\n",
    "for i, document in enumerate(documents):\n",
    "    print(f\"Document {i + 1}:\")\n",
    "    for j, term in enumerate(terms):\n",
    "        tfidf_score = tfidf_matrix[i, j]\n",
    "        if tfidf_score > 0:\n",
    "            print(f\"    {term}: {tfidf_score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "    cat: 0.341\n",
      "    mat: 0.448\n",
      "    on: 0.448\n",
      "    sat: 0.448\n",
      "    the: 0.530\n",
      "Document 2:\n",
      "    dog: 0.341\n",
      "    in: 0.448\n",
      "    played: 0.448\n",
      "    the: 0.530\n",
      "    yard: 0.448\n",
      "Document 3:\n",
      "    and: 0.424\n",
      "    are: 0.424\n",
      "    cat: 0.323\n",
      "    dog: 0.323\n",
      "    friends: 0.424\n",
      "    the: 0.501\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Example documents\n",
    "documents = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog played in the yard.\",\n",
    "    \"The cat and the dog are friends.\"\n",
    "]\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the documents and transform them to TF-IDF matrix\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names (terms)\n",
    "terms = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print TF-IDF scores for each term in each document\n",
    "for i, document in enumerate(documents):\n",
    "    print(f\"Document {i + 1}:\")\n",
    "    for j, term in enumerate(terms):\n",
    "        tfidf_score = tfidf_matrix[i, j]\n",
    "        if tfidf_score > 0:\n",
    "            print(f\"    {term}: {tfidf_score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of words in the Corpus :  13\n",
      "The Words in the corpus: \n",
      " {'played', 'dog', 'and', 'friends.', 'yard.', 'on', 'are', 'sat', 'The', 'cat', 'mat.', 'in', 'the'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "documents = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog played in the yard.\",\n",
    "    \"The cat and the dog are friends.\"\n",
    "]\n",
    "\n",
    "words_set = set()\n",
    "\n",
    "for doc in documents:\n",
    "  words = doc.split(\" \")\n",
    "  words_set = words_set.union(set(words))\n",
    "\n",
    "print(\"Number Of words in the Corpus : \",len(words_set))\n",
    "print(\"The Words in the corpus: \\n\", words_set)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "columns cannot be a set",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6132\\2938272410.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mn_docs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mn_words_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf_tf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_docs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_words_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwords_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_docs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m   \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\mistr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    729\u001b[0m         \u001b[1;31m# GH47215\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"index cannot be a set\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 733\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"columns cannot be a set\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    734\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: columns cannot be a set"
     ]
    }
   ],
   "source": [
    "n_docs = len(documents)\n",
    "n_words_set = len(words_set)\n",
    "\n",
    "df_tf = pd.DataFrame(np.zeros((n_docs, n_words_set)), columns=words_set)\n",
    "\n",
    "for i in range(n_docs):\n",
    "  words = corpus[i].split(\" \")\n",
    "  for w in words:\n",
    "    df_tf[w][i] = df_tf[w][i] + (1 / len(words))\n",
    "\n",
    "df_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the Corpus: 13\n",
      "The words in the corpus:\n",
      " {'played', 'dog', 'and', 'friends.', 'yard.', 'on', 'are', 'sat', 'The', 'cat', 'mat.', 'in', 'the'}\n",
      "\n",
      "Term Frequency (TF) DataFrame:\n",
      "     played       dog       and  friends.     yard.        on       are  \\\n",
      "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.166667  0.000000   \n",
      "1  0.166667  0.166667  0.000000  0.000000  0.166667  0.000000  0.000000   \n",
      "2  0.000000  0.142857  0.142857  0.142857  0.000000  0.000000  0.142857   \n",
      "\n",
      "        sat       The       cat      mat.        in       the  \n",
      "0  0.166667  0.166667  0.166667  0.166667  0.000000  0.166667  \n",
      "1  0.000000  0.166667  0.000000  0.000000  0.166667  0.166667  \n",
      "2  0.000000  0.142857  0.142857  0.000000  0.000000  0.142857  \n",
      "IDF of: \n",
      "         played: 0.47712125471966244\n",
      "            dog: 0.17609125905568124\n",
      "            and: 0.47712125471966244\n",
      "       friends.: 0.47712125471966244\n",
      "          yard.: 0.47712125471966244\n",
      "             on: 0.47712125471966244\n",
      "            are: 0.47712125471966244\n",
      "            sat: 0.47712125471966244\n",
      "            The:        0.0\n",
      "            cat: 0.17609125905568124\n",
      "           mat.: 0.47712125471966244\n",
      "             in: 0.47712125471966244\n",
      "            the:        0.0\n",
      "<class 'scipy.sparse._csr.csr_matrix'> (3, 12)\n",
      "[[0.         0.         0.34101521 0.         0.         0.\n",
      "  0.44839402 0.44839402 0.         0.44839402 0.52965746 0.        ]\n",
      " [0.         0.         0.         0.34101521 0.         0.44839402\n",
      "  0.         0.         0.44839402 0.         0.52965746 0.44839402]\n",
      " [0.42439575 0.42439575 0.32276391 0.32276391 0.42439575 0.\n",
      "  0.         0.         0.         0.         0.50130994 0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mistr\\AppData\\Local\\Temp\\ipykernel_6132\\2994320423.py:58: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df_tf_idf[w][i] = df_tf[w][i] * idf[w]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "documents = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog played in the yard.\",\n",
    "    \"The cat and the dog are friends.\"\n",
    "]\n",
    "\n",
    "words_set = set()\n",
    "\n",
    "# Create a set of unique words in the corpus\n",
    "for doc in documents:\n",
    "    words = doc.split(\" \")\n",
    "    words_set = words_set.union(set(words))\n",
    "\n",
    "print(\"Number of words in the Corpus:\", len(words_set))\n",
    "print(\"The words in the corpus:\\n\", words_set)\n",
    "\n",
    "n_docs = len(documents)\n",
    "n_words_set = len(words_set)\n",
    "\n",
    "# Convert the set of words into a list\n",
    "words_list = list(words_set)\n",
    "\n",
    "# Initialize a DataFrame to store TF values\n",
    "df_tf = pd.DataFrame(np.zeros((n_docs, n_words_set)), columns=words_list)\n",
    "\n",
    "# Calculate TF for each term in each document\n",
    "for i in range(n_docs):\n",
    "    words = documents[i].split(\" \")\n",
    "    word_count = len(words)\n",
    "    for word in words:\n",
    "        df_tf.at[i, word] += 1 / word_count\n",
    "\n",
    "print(\"\\nTerm Frequency (TF) DataFrame:\")\n",
    "print(df_tf)\n",
    "\n",
    "print(\"IDF of: \")\n",
    "\n",
    "idf = {}\n",
    "\n",
    "for w in words_set:\n",
    "    k = 0    # number of documents in the corpus that contain this word\n",
    "    \n",
    "    for i in range(n_docs):\n",
    "        if w in documents[i].split():\n",
    "            k += 1\n",
    "            \n",
    "    idf[w] =  np.log10(n_docs / k)\n",
    "    \n",
    "    print(f'{w:>15}: {idf[w]:>10}' )\n",
    "\n",
    "df_tf_idf = df_tf.copy()\n",
    "\n",
    "for w in words_set:\n",
    "    for i in range(n_docs):\n",
    "        df_tf_idf[w][i] = df_tf[w][i] * idf[w]\n",
    "        \n",
    "df_tf_idf\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tr_idf_model  = TfidfVectorizer()\n",
    "tf_idf_vector = tr_idf_model.fit_transform(documents)\n",
    "\n",
    "print(type(tf_idf_vector), tf_idf_vector.shape)\n",
    "\n",
    "tf_idf_array = tf_idf_vector.toarray()\n",
    "\n",
    "print(tf_idf_array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the Corpus: 13\n",
      "The words in the corpus:\n",
      " {'played', 'dog', 'and', 'friends.', 'yard.', 'on', 'are', 'sat', 'The', 'cat', 'mat.', 'in', 'the'}\n",
      "\n",
      "Term Frequency (TF) DataFrame:\n",
      "     played       dog       and  friends.     yard.        on       are  \\\n",
      "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.166667  0.000000   \n",
      "1  0.166667  0.166667  0.000000  0.000000  0.166667  0.000000  0.000000   \n",
      "2  0.000000  0.142857  0.142857  0.142857  0.000000  0.000000  0.142857   \n",
      "\n",
      "        sat       The       cat      mat.        in       the  \n",
      "0  0.166667  0.166667  0.166667  0.166667  0.000000  0.166667  \n",
      "1  0.000000  0.166667  0.000000  0.000000  0.166667  0.166667  \n",
      "2  0.000000  0.142857  0.142857  0.000000  0.000000  0.142857  \n",
      "\n",
      "Inverse Document Frequency (IDF) DataFrame:\n",
      "     played  dog       and  friends.     yard.        on       are       sat  \\\n",
      "0  0.405465  0.0  0.405465  0.405465  0.405465  0.405465  0.405465  0.405465   \n",
      "\n",
      "        The  cat      mat.        in       the  \n",
      "0 -0.287682  0.0  0.405465  0.405465 -0.287682  \n",
      "\n",
      "TF-IDF Scores DataFrame:\n",
      "     played  dog       and  friends.     yard.        on       are       sat  \\\n",
      "0  0.000000  0.0  0.000000  0.000000  0.000000  0.067578  0.000000  0.067578   \n",
      "1  0.067578  0.0  0.000000  0.000000  0.067578  0.000000  0.000000  0.000000   \n",
      "2  0.000000  0.0  0.057924  0.057924  0.000000  0.000000  0.057924  0.000000   \n",
      "\n",
      "        The  cat      mat.        in       the  \n",
      "0 -0.047947  0.0  0.067578  0.000000 -0.047947  \n",
      "1 -0.047947  0.0  0.000000  0.067578 -0.047947  \n",
      "2 -0.041097  0.0  0.000000  0.000000 -0.041097  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "documents = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog played in the yard.\",\n",
    "    \"The cat and the dog are friends.\"\n",
    "]\n",
    "\n",
    "words_set = set()\n",
    "\n",
    "# Create a set of unique words in the corpus\n",
    "for doc in documents:\n",
    "    words = doc.split(\" \")\n",
    "    words_set = words_set.union(set(words))\n",
    "\n",
    "print(\"Number of words in the Corpus:\", len(words_set))\n",
    "print(\"The words in the corpus:\\n\", words_set)\n",
    "\n",
    "n_docs = len(documents)\n",
    "n_words_set = len(words_set)\n",
    "\n",
    "# Convert the set of words into a list\n",
    "words_list = list(words_set)\n",
    "\n",
    "# Initialize a DataFrame to store TF values\n",
    "df_tf = pd.DataFrame(np.zeros((n_docs, n_words_set)), columns=words_list)\n",
    "\n",
    "# Calculate TF for each term in each document\n",
    "for i in range(n_docs):\n",
    "    words = documents[i].split(\" \")\n",
    "    word_count = len(words)\n",
    "    for word in words:\n",
    "        df_tf.at[i, word] += 1 / word_count\n",
    "\n",
    "print(\"\\nTerm Frequency (TF) DataFrame:\")\n",
    "print(df_tf)\n",
    "\n",
    "# Initialize a DataFrame to store IDF values\n",
    "df_idf = pd.DataFrame(np.zeros((1, n_words_set)), columns=words_list)\n",
    "\n",
    "# Calculate IDF for each term\n",
    "for word in words_list:\n",
    "    doc_count = sum([1 for doc in documents if word in doc])\n",
    "    df_idf.at[0, word] = math.log(n_docs / (1 + doc_count))\n",
    "\n",
    "print(\"\\nInverse Document Frequency (IDF) DataFrame:\")\n",
    "print(df_idf)\n",
    "\n",
    "# Calculate TF-IDF scores\n",
    "df_tfidf = df_tf * df_idf.values\n",
    "\n",
    "print(\"\\nTF-IDF Scores DataFrame:\")\n",
    "print(df_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the Corpus: 13\n",
      "The words in the corpus:\n",
      " {'played', 'dog', 'and', 'friends.', 'yard.', 'on', 'are', 'sat', 'The', 'cat', 'mat.', 'in', 'the'}\n",
      "\n",
      "Term Frequency (TF) DataFrame:\n",
      "     played       dog       and  friends.     yard.        on       are  \\\n",
      "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.166667  0.000000   \n",
      "1  0.166667  0.166667  0.000000  0.000000  0.166667  0.000000  0.000000   \n",
      "2  0.000000  0.142857  0.142857  0.142857  0.000000  0.000000  0.142857   \n",
      "\n",
      "        sat       The       cat      mat.        in       the  \n",
      "0  0.166667  0.166667  0.166667  0.166667  0.000000  0.166667  \n",
      "1  0.000000  0.166667  0.000000  0.000000  0.166667  0.166667  \n",
      "2  0.000000  0.142857  0.142857  0.000000  0.000000  0.142857  \n",
      "\n",
      "TF-IDF Scores Array:\n",
      "[[0.         0.         0.34101521 0.         0.         0.\n",
      "  0.44839402 0.44839402 0.         0.44839402 0.52965746 0.        ]\n",
      " [0.         0.         0.         0.34101521 0.         0.44839402\n",
      "  0.         0.         0.44839402 0.         0.52965746 0.44839402]\n",
      " [0.42439575 0.42439575 0.32276391 0.32276391 0.42439575 0.\n",
      "  0.         0.         0.         0.         0.50130994 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog played in the yard.\",\n",
    "    \"The cat and the dog are friends.\"\n",
    "]\n",
    "\n",
    "# Create a set of unique words in the corpus\n",
    "words_set = set()\n",
    "for doc in documents:\n",
    "    words = doc.split(\" \")\n",
    "    words_set = words_set.union(set(words))\n",
    "\n",
    "# Display the number of unique words and the words in the corpus\n",
    "print(\"Number of words in the Corpus:\", len(words_set))\n",
    "print(\"The words in the corpus:\\n\", words_set)\n",
    "\n",
    "n_docs = len(documents)\n",
    "n_words_set = len(words_set)\n",
    "\n",
    "# Convert the set of words into a list\n",
    "words_list = list(words_set)\n",
    "\n",
    "# Initialize a DataFrame to store Term Frequency (TF) values\n",
    "df_tf = pd.DataFrame(np.zeros((n_docs, n_words_set)), columns=words_list)\n",
    "\n",
    "# Calculate TF for each term in each document\n",
    "for i in range(n_docs):\n",
    "    words = documents[i].split(\" \")\n",
    "    word_count = len(words)\n",
    "    for word in words:\n",
    "        df_tf.at[i, word] += 1 / word_count\n",
    "\n",
    "# Display the Term Frequency (TF) DataFrame\n",
    "print(\"\\nTerm Frequency (TF) DataFrame:\")\n",
    "print(df_tf)\n",
    "\n",
    "# Initialize a TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the documents and transform them into TF-IDF vectors\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert TF-IDF vectors to an array for display\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "# Display the TF-IDF array\n",
    "print(\"\\nTF-IDF Scores Array:\")\n",
    "print(tfidf_array)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the corpus: 13\n",
      "The words in the corpus: \n",
      " {'played', 'dog', 'and', 'friends.', 'yard.', 'on', 'are', 'sat', 'The', 'cat', 'mat.', 'in', 'the'}\n",
      "\n",
      "Term Frequency (TF) DataFrame:\n",
      "     played       dog       and  friends.     yard.        on       are  \\\n",
      "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.166667  0.000000   \n",
      "1  0.166667  0.166667  0.000000  0.000000  0.166667  0.000000  0.000000   \n",
      "2  0.000000  0.142857  0.142857  0.142857  0.000000  0.000000  0.142857   \n",
      "\n",
      "        sat       The       cat      mat.        in       the  \n",
      "0  0.166667  0.166667  0.166667  0.166667  0.000000  0.166667  \n",
      "1  0.000000  0.166667  0.000000  0.000000  0.166667  0.166667  \n",
      "2  0.000000  0.142857  0.142857  0.000000  0.000000  0.142857  \n",
      "\n",
      "IDF of:\n",
      "         played: 0.47712125471966244\n",
      "            dog: 0.17609125905568124\n",
      "            and: 0.47712125471966244\n",
      "       friends.: 0.47712125471966244\n",
      "          yard.: 0.47712125471966244\n",
      "             on: 0.47712125471966244\n",
      "            are: 0.47712125471966244\n",
      "            sat: 0.47712125471966244\n",
      "            The:        0.0\n",
      "            cat: 0.17609125905568124\n",
      "           mat.: 0.47712125471966244\n",
      "             in: 0.47712125471966244\n",
      "            the:        0.0\n",
      "\n",
      "TF-IDF DataFrame (Computed using TfidfVectorizer):\n",
      "        and       are       cat       dog   friends        in       mat  \\\n",
      "0  0.000000  0.000000  0.341015  0.000000  0.000000  0.000000  0.448394   \n",
      "1  0.000000  0.000000  0.000000  0.341015  0.000000  0.448394  0.000000   \n",
      "2  0.424396  0.424396  0.322764  0.322764  0.424396  0.000000  0.000000   \n",
      "\n",
      "         on    played       sat       the      yard  \n",
      "0  0.448394  0.000000  0.448394  0.529657  0.000000  \n",
      "1  0.000000  0.448394  0.000000  0.529657  0.448394  \n",
      "2  0.000000  0.000000  0.000000  0.501310  0.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog played in the yard.\",\n",
    "    \"The cat and the dog are friends.\"\n",
    "]\n",
    "\n",
    "words_set = set()\n",
    "\n",
    "# Create a set of unique words in the corpus\n",
    "for doc in documents:\n",
    "    words = doc.split(\" \")\n",
    "    words_set = words_set.union(set(words))\n",
    "\n",
    "# Convert the set of words into a list\n",
    "words_list = list(words_set)\n",
    "\n",
    "print('Number of words in the corpus:', len(words_set))\n",
    "print('The words in the corpus: \\n', words_set)\n",
    "\n",
    "n_docs = len(documents)         # Number of documents in the corpus\n",
    "n_words_set = len(words_set)    # Number of unique words in the corpus\n",
    "\n",
    "# Initialize DataFrame to store Term Frequency (TF) values\n",
    "df_tf = pd.DataFrame(np.zeros((n_docs, n_words_set)), columns=words_list)\n",
    "\n",
    "# Compute Term Frequency (TF)\n",
    "for i in range(n_docs):\n",
    "    words = documents[i].split(' ')  # Words in the document\n",
    "    for w in words:\n",
    "        df_tf.at[i, w] += 1 / len(words)\n",
    "\n",
    "print(\"\\nTerm Frequency (TF) DataFrame:\")\n",
    "print(df_tf)\n",
    "\n",
    "print(\"\\nIDF of:\")\n",
    "idf = {}\n",
    "\n",
    "# Compute Inverse Document Frequency (IDF)\n",
    "for w in words_list:\n",
    "    k = sum([1 for doc in documents if w in doc.split()])\n",
    "    idf[w] = np.log10(n_docs / k)\n",
    "    print(f'{w:>15}: {idf[w]:>10}')\n",
    "\n",
    "# Initialize a TF-IDF DataFrame\n",
    "df_tf_idf = df_tf.copy()\n",
    "\n",
    "# # Compute TF-IDF scores manually\n",
    "# for w in words_list:\n",
    "#     for i in range(n_docs):\n",
    "#         df_tf_idf.at[i, w] *= idf[w]\n",
    "\n",
    "# print(\"\\nTF-IDF DataFrame (Manually Calculated):\")\n",
    "# print(df_tf_idf)\n",
    "\n",
    "# Use TfidfVectorizer to compute TF-IDF scores\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "words_set_sklearn = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "df_tf_idf_sklearn = pd.DataFrame(tfidf_array, columns=words_set_sklearn)\n",
    "\n",
    "print(\"\\nTF-IDF DataFrame (Computed using TfidfVectorizer):\")\n",
    "print(df_tf_idf_sklearn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the Corpus: 13\n",
      "The words in the corpus:\n",
      " {'played', 'dog', 'and', 'friends.', 'yard.', 'on', 'are', 'sat', 'The', 'cat', 'mat.', 'in', 'the'}\n",
      "\n",
      "Term Frequency (TF) DataFrame:\n",
      "     played       dog       and  friends.     yard.        on       are  \\\n",
      "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.166667  0.000000   \n",
      "1  0.166667  0.166667  0.000000  0.000000  0.166667  0.000000  0.000000   \n",
      "2  0.000000  0.142857  0.142857  0.142857  0.000000  0.000000  0.142857   \n",
      "\n",
      "        sat       The       cat      mat.        in       the  \n",
      "0  0.166667  0.166667  0.166667  0.166667  0.000000  0.166667  \n",
      "1  0.000000  0.166667  0.000000  0.000000  0.166667  0.166667  \n",
      "2  0.000000  0.142857  0.142857  0.000000  0.000000  0.142857  \n",
      "\n",
      "Inverse Document Frequency (IDF) DataFrame:\n",
      "     played  dog       and  friends.     yard.        on       are       sat  \\\n",
      "0  0.405465  0.0  0.405465  0.405465  0.405465  0.405465  0.405465  0.405465   \n",
      "\n",
      "        The  cat      mat.        in       the  \n",
      "0 -0.287682  0.0  0.405465  0.405465 -0.287682  \n",
      "\n",
      "TF-IDF Scores DataFrame:\n",
      "     played  dog       and  friends.     yard.        on       are       sat  \\\n",
      "0  0.000000  0.0  0.000000  0.000000  0.000000  0.067578  0.000000  0.067578   \n",
      "1  0.067578  0.0  0.000000  0.000000  0.067578  0.000000  0.000000  0.000000   \n",
      "2  0.000000  0.0  0.057924  0.057924  0.000000  0.000000  0.057924  0.000000   \n",
      "\n",
      "        The  cat      mat.        in       the  \n",
      "0 -0.047947  0.0  0.067578  0.000000 -0.047947  \n",
      "1 -0.047947  0.0  0.000000  0.067578 -0.047947  \n",
      "2 -0.041097  0.0  0.000000  0.000000 -0.041097  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "documents = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog played in the yard.\",\n",
    "    \"The cat and the dog are friends.\"\n",
    "]\n",
    "\n",
    "words_set = set()\n",
    "\n",
    "# Create a set of unique words in the corpus\n",
    "for doc in documents:\n",
    "    words = doc.split(\" \")\n",
    "    words_set = words_set.union(set(words))\n",
    "\n",
    "print(\"Number of words in the Corpus:\", len(words_set))\n",
    "print(\"The words in the corpus:\\n\", words_set)\n",
    "\n",
    "n_docs = len(documents)\n",
    "n_words_set = len(words_set)\n",
    "\n",
    "# Convert the set of words into a list\n",
    "words_list = list(words_set)\n",
    "\n",
    "# Initialize a DataFrame to store TF values\n",
    "df_tf = pd.DataFrame(np.zeros((n_docs, n_words_set)), columns=words_list)\n",
    "\n",
    "# Calculate TF for each term in each document\n",
    "for i in range(n_docs):\n",
    "    words = documents[i].split(\" \")\n",
    "    word_count = len(words)\n",
    "    for word in words:\n",
    "        df_tf.at[i, word] += 1 / word_count\n",
    "\n",
    "print(\"\\nTerm Frequency (TF) DataFrame:\")\n",
    "print(df_tf)\n",
    "\n",
    "# Initialize a DataFrame to store IDF values\n",
    "df_idf = pd.DataFrame(np.zeros((1, n_words_set)), columns=words_list)\n",
    "\n",
    "# Calculate IDF for each term\n",
    "for word in words_list:\n",
    "    doc_count = sum([1 for doc in documents if word in doc])\n",
    "    df_idf.at[0, word] = math.log(n_docs / (1 + doc_count))\n",
    "\n",
    "print(\"\\nInverse Document Frequency (IDF) DataFrame:\")\n",
    "print(df_idf)\n",
    "\n",
    "# Calculate TF-IDF scores\n",
    "df_tfidf = df_tf * df_idf.values\n",
    "\n",
    "print(\"\\nTF-IDF Scores DataFrame:\")\n",
    "print(df_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the corpus: 13\n",
      "The words in the corpus: \n",
      " {'played', 'dog', 'and', 'friends.', 'yard.', 'on', 'are', 'sat', 'The', 'cat', 'mat.', 'in', 'the'}\n",
      "\n",
      "Term Frequency (TF) DataFrame:\n",
      "     played       dog       and  friends.     yard.        on       are  \\\n",
      "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.166667  0.000000   \n",
      "1  0.166667  0.166667  0.000000  0.000000  0.166667  0.000000  0.000000   \n",
      "2  0.000000  0.142857  0.142857  0.142857  0.000000  0.000000  0.142857   \n",
      "\n",
      "        sat       The       cat      mat.        in       the  \n",
      "0  0.166667  0.166667  0.166667  0.166667  0.000000  0.166667  \n",
      "1  0.000000  0.166667  0.000000  0.000000  0.166667  0.166667  \n",
      "2  0.000000  0.142857  0.142857  0.000000  0.000000  0.142857  \n",
      "\n",
      "IDF(Inverse Document Frequency) of:\n",
      "         played: 0.47712125471966244\n",
      "            dog: 0.17609125905568124\n",
      "            and: 0.47712125471966244\n",
      "       friends.: 0.47712125471966244\n",
      "          yard.: 0.47712125471966244\n",
      "             on: 0.47712125471966244\n",
      "            are: 0.47712125471966244\n",
      "            sat: 0.47712125471966244\n",
      "            The:        0.0\n",
      "            cat: 0.17609125905568124\n",
      "           mat.: 0.47712125471966244\n",
      "             in: 0.47712125471966244\n",
      "            the:        0.0\n",
      "\n",
      "TF-IDF DataFrame (Computed using TfidfVectorizer):\n",
      "        and       are       cat       dog   friends        in       mat  \\\n",
      "0  0.000000  0.000000  0.341015  0.000000  0.000000  0.000000  0.448394   \n",
      "1  0.000000  0.000000  0.000000  0.341015  0.000000  0.448394  0.000000   \n",
      "2  0.424396  0.424396  0.322764  0.322764  0.424396  0.000000  0.000000   \n",
      "\n",
      "         on    played       sat       the      yard  \n",
      "0  0.448394  0.000000  0.448394  0.529657  0.000000  \n",
      "1  0.000000  0.448394  0.000000  0.529657  0.448394  \n",
      "2  0.000000  0.000000  0.000000  0.501310  0.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog played in the yard.\",\n",
    "    \"The cat and the dog are friends.\"\n",
    "]\n",
    "\n",
    "words_set = set()\n",
    "\n",
    "# Create a set of unique words in the corpus\n",
    "for doc in documents:\n",
    "    words = doc.split(\" \")\n",
    "    words_set = words_set.union(set(words))\n",
    "\n",
    "# Convert the set of words into a list\n",
    "words_list = list(words_set)\n",
    "\n",
    "print('Number of words in the corpus:', len(words_set))\n",
    "print('The words in the corpus: \\n', words_set)\n",
    "\n",
    "n_docs = len(documents)         # Number of documents in the corpus\n",
    "n_words_set = len(words_set)    # Number of unique words in the corpus\n",
    "\n",
    "# Initialize DataFrame to store Term Frequency (TF) values\n",
    "df_tf = pd.DataFrame(np.zeros((n_docs, n_words_set)), columns=words_list)\n",
    "\n",
    "# Compute Term Frequency (TF)\n",
    "for i in range(n_docs):\n",
    "    words = documents[i].split(' ')  # Words in the document\n",
    "    for w in words:\n",
    "        df_tf.at[i, w] += 1 / len(words)\n",
    "\n",
    "print(\"\\nTerm Frequency (TF) DataFrame:\")\n",
    "print(df_tf)\n",
    "\n",
    "print(\"\\nIDF(Inverse Document Frequency) of:\")\n",
    "idf = {}\n",
    "\n",
    "# Compute Inverse Document Frequency (IDF)\n",
    "for w in words_list:\n",
    "    k = sum([1 for doc in documents if w in doc.split()])\n",
    "    idf[w] = np.log10(n_docs / k)\n",
    "    print(f'{w:>15}: {idf[w]:>10}')\n",
    "\n",
    "# Initialize a TF-IDF DataFrame\n",
    "df_tf_idf = df_tf.copy()\n",
    "\n",
    "# Use TfidfVectorizer to compute TF-IDF scores\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "words_set_sklearn = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "df_tf_idf_sklearn = pd.DataFrame(tfidf_array, columns=words_set_sklearn)\n",
    "\n",
    "print(\"\\nTF-IDF DataFrame (Computed using TfidfVectorizer):\")\n",
    "print(df_tf_idf_sklearn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural language processing (NLP) is a fascinating field.', 'It deals with how computers understand and interact with human language.', 'Sentence tokenization is one of the basic tasks in NLP.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Natural language processing (NLP) is a fascinating field. It deals with how computers understand and interact with human language. Sentence tokenization is one of the basic tasks in NLP.\"\n",
    "\n",
    "sentence = sent_tokenize(text)\n",
    "\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', '.', 'It', 'deals', 'with', 'how', 'computers', 'understand', 'and', 'interact', 'with', 'human', 'language', '.', 'Sentence', 'tokenization', 'is', 'one', 'of', 'the', 'basic', 'tasks', 'in', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Natural language processing (NLP) is a fascinating field. It deals with how computers understand and interact with human language. Sentence tokenization is one of the basic tasks in NLP.\"\n",
    "\n",
    "word = word_tokenize(text)\n",
    "\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'basque',\n",
       " 'bengali',\n",
       " 'catalan',\n",
       " 'chinese',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hebrew',\n",
       " 'hinglish',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ----> eat\n",
      "eats ----> eat\n",
      "eaten ----> eaten\n",
      "writing ----> write\n",
      "writes ----> write\n",
      "programming ----> program\n",
      "programs ----> program\n",
      "history ----> histori\n",
      "running ----> run\n",
      "cats ----> cat\n",
      "jumped ----> jump\n",
      "faster ----> faster\n",
      "quickly ----> quickli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "words = [\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"running\", \"cats\", \"jumped\", \"faster\", \"quickly\"]\n",
    "\n",
    "stemming = PorterStemmer()\n",
    "\n",
    "for word in words:\n",
    "  print(word+\" ----> \"+stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ---> eat\n",
      "eats ---> eat\n",
      "eaten ---> eaten\n",
      "writing ---> write\n",
      "writes ---> write\n",
      "programming ---> program\n",
      "programs ---> program\n",
      "history ---> histori\n",
      "running ---> run\n",
      "cats ---> cat\n",
      "jumped ---> jump\n",
      "faster ---> faster\n",
      "quickly ---> quick\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stammer = SnowballStemmer(\"english\")\n",
    "\n",
    "words = [\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"running\", \"cats\", \"jumped\", \"faster\", \"quickly\"]\n",
    "\n",
    "for word in words:\n",
    "  print(word+\" ---> \"+stammer.stem(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ---> eat\n",
      "eats ---> eat\n",
      "eaten ---> eaten\n",
      "writing ---> writ\n",
      "writes ---> write\n",
      "programming ---> programm\n",
      "programs ---> program\n",
      "history ---> history\n",
      "running ---> runn\n",
      "cats ---> cat\n",
      "jumped ---> jump\n",
      "faster ---> fast\n",
      "quickly ---> quick\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "words = [\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"running\", \"cats\", \"jumped\", \"faster\", \"quickly\"]\n",
    "\n",
    "# Define stemming rules using regular expressions\n",
    "pattern = r\"(ing$|s$|ed$|er$|est$|ly$)\"\n",
    "\n",
    "regexp_stemmer = RegexpStemmer(pattern)\n",
    "\n",
    "for word in words:\n",
    "  print(word+\" ---> \"+regexp_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Verbs:\n",
      "Original: running\t Lemma: run\n",
      "Original: went\t Lemma: go\n",
      "Original: eating\t Lemma: eat\n",
      "\n",
      "Lemmatized Adjectives:\n",
      "Original: better\t Lemma: good\n",
      "Original: worst\t Lemma: bad\n",
      "Original: faster\t Lemma: fast\n",
      "\n",
      "Lemmatized Adverbs:\n",
      "Original: quickly\t Lemma: quickly\n",
      "Original: slowly\t Lemma: slowly\n",
      "Original: hardly\t Lemma: hardly\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Create an instance of WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Examples of words categorized as verbs\n",
    "verbs = [\"running\", \"went\", \"eating\"]\n",
    "\n",
    "# Examples of words categorized as adjectives\n",
    "adjectives = [\"better\", \"worst\", \"faster\"]\n",
    "\n",
    "# Examples of words categorized as adverbs\n",
    "adverbs = [\"quickly\", \"slowly\", \"hardly\"]\n",
    "\n",
    "# Lemmatize verbs\n",
    "print(\"Lemmatized Verbs:\")\n",
    "for verb in verbs:\n",
    "    lemma = lemmatizer.lemmatize(verb, pos='v')  # 'v' indicates verb\n",
    "    print(f\"Original: {verb}\\t Lemma: {lemma}\")\n",
    "\n",
    "# Lemmatize adjectives\n",
    "print(\"\\nLemmatized Adjectives:\")\n",
    "for adjective in adjectives:\n",
    "    lemma = lemmatizer.lemmatize(adjective, pos='a')  # 'a' indicates adjective\n",
    "    print(f\"Original: {adjective}\\t Lemma: {lemma}\")\n",
    "\n",
    "# Lemmatize adverbs\n",
    "print(\"\\nLemmatized Adverbs:\")\n",
    "for adverb in adverbs:\n",
    "    lemma = lemmatizer.lemmatize(adverb, pos='r')  # 'r' indicates adverb\n",
    "    print(f\"Original: {adverb}\\t Lemma: {lemma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boyfriend\n",
      "birthday\n",
      "please\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "class WordReplacer:\n",
    "  def __init__(self,word_map):\n",
    "    self.word_map = word_map\n",
    "\n",
    "  def replace(self,word):\n",
    "    return self.word_map.get(word,word)\n",
    "  \n",
    "replacer = WordReplacer({\"bday\" : \"Birthday\"})\n",
    "replacer.replace(\"bday\")\n",
    "\n",
    "class CsvWordReplacer(WordReplacer):\n",
    "    def __init__(self, fname):\n",
    "        word_map = {}\n",
    "        with open(fname, 'r') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            for row in reader:\n",
    "                word = row['Word']\n",
    "                synonyms = row['Synonyms']\n",
    "                word_map[word] = synonyms\n",
    "        super().__init__(word_map)\n",
    "\n",
    "replacer = CsvWordReplacer(\"Synonyms.csv\")\n",
    "\n",
    "text = replacer.replace(\"bf\")\n",
    "text1 = replacer.replace(\"bday\")\n",
    "text2 = replacer.replace(\"pls\")\n",
    "\n",
    "print(text)\n",
    "print(text1)\n",
    "print(text2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
